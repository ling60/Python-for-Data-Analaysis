对于我来说，完成论文的数据抓取部分曲折又艰辛。开始花了很久时间做出了抓取了智
联招聘的数据（没错，我本来是要抓那个网站的，那个网站有3800 条数据可以用！）。然
后完成了程序，测试了程序运行良好，可以抓的到数据，但是并没有存数据的我就开心的把
它放到了一边，去做其他的事。再过几天准备开始抓数据了，发现抓不出来。自己并没有做
任何的改动所以很奇怪为什么出现这种情况，一步一步的print 找错误，发现网站显示没有
数据可以抓取，最后在几天抱着希望又不断经历失望的情况下，绝望的确定是对方服务器出
现了问题。重头再来这四个字有时候带着一些东山再起的期待，有的时候是无奈。因为没有
别的办法，我决定改掉程序，换一个网站重新抓取。有的人会说，不就是再编个程序吗？没
有几行代码。可是对于我来说，很难。爬虫是从网上自学的，基础并不扎实，对一些方法并
没有透彻的理解，而且对网页的解析也没有学的太明白。只能看别人的代码，学别人的思路
然后模仿着写一写。也会出现很多问题。别人的代码可能用的是Python2.X 版本，很多包或
者用法都有了很大的改动，而且可能也会有错误，也有各种各样的思路。不会就上网去搜，
去问才能学会。包括在后面终于抓到了数据，但是处理数据也会出现很多问题。比如抓取到
的现提取到的工资数据有三种单位（万/年万/月千/月），无法统计，首先想的是我怎么
统一单位，怎么使用切片和循环才能取出自己想要的东西。所有看似简单的问题都不简单，
有很多细节要考虑。只能不断试错，不断思考。这里有一个特有意思的故事，我拎着垃圾准
备丢到一楼的垃圾桶，丢完出去吃饭。下楼的过程里脑子里一直在想我处理完的薪资数据怎
么会有两万多条，源数据600 条啊，哪里出问题了？于是一直提着垃圾走到吃饭的地方。低
头一看，我怎么还拎着它。回来继续处理这个问题，分开print，发现数据全部变成了源数
据数量的平方倍，发现我进行了两次for 循环。好不容易解决了这个问题，发现处理完有的
数据居然是负数，又一步一步检查，发现少加了一组括号。有时候你的程序没报错，但数据
会出错，每一步都要检查一下。老实说，问题不断出现真的让人头大，但是自己能够凭借自
己的力量，一个一个的解决那种成就感真的是做其他事情不会有的。这个论文、这门课程让
我看见了自己的一点可能性。原来我在思考方面很懒，总想找到现成的东西。可是当我开始，
甚至是不得不开始自己思考，自己解决问题的时候，我发现这种感觉太好了。我会闲下来就
主动想程序里的问题，这种积极性高考后已经三年没出现过了。很难说清楚吸引我的是什么，
也许是主动创造、探索而不是被动接受吧。我想，在大三的尾巴上，我终于找到了自己的一
点点热情所在。在经历了编程的小折磨之后，还对数据感兴趣，也许就是真爱了。
下面是我的一点点经验：
1.不要拖不要拖不要拖！你永远不知道会发生什么，能做完的时候就抓紧时间做。
2.要多动脑。有的时候，你会习惯性的犯懒，总想找网上现成的，总想依靠别人的帮助，
尝试开始自己思考，你就会发现自己解决问题的体验太好了。你会爱上这种感觉的。
3.不要着急。一步一步慢慢来，你要相信，问题都会解决的，急不是解决问题的方法。




在这个项目中我学会了简单的爬虫（真的是很简单很简单了），还有怎么对中文分词统计词频，制作词云，可视化图表的修饰等。我觉得一个项目最重要就是选题，一是要有价值吸引别人，二是要自己感兴趣，我的这次选题比较仓促，实施起来的确很简单，只能自己多花时间完善代码和图表，在写论文时由于选题不好真的不知道该怎么写下去，还好应该算写完了吧。。。。我觉得Python是个很有用的课程，如果学弟学妹有自己感兴趣的项目建议多看网课，学学怎么爬虫，完善自己的论文，如果能结合自己的专业就更好了，有一篇论文拿出来都会很出彩的。还有对于这种需要自学的课程，自己做项目千万不能拖，自己下去学网课也是自制力的一种证明方式吧，如果拖着论文一直不写，就会像我一样，在期末考的时候复习论文焦头烂额，一定要按照自己的研究时间安排来做。对于一开始的选题一定要选好，不然到后面真的很麻烦，还是不要像我一样不知道怎么选题，最后硬着头皮写了下去。如果要选类似的文本分析建议多找点数据资料分析，多找参考文献，模仿别人的写论文思路，不要只是简单的文本分析，做好能结合统计之类的知识。


1.爬虫趁早学：类似行业分析这种的话，数据是Basic，所以一定一定要在起码Part1的时候就可以扒简单的网站最好！
爬虫推荐课程：MOOC的北京理工大学的《Python网络爬虫与信息提取》
这个从基础的request beautifulsoup库讲起，然后后面会有实例分析，然后最后也有一个和60老师一样的大作业，但是这个大作业比老师的大作业提前了两个月，课程有群有助教，跟下来可以学会很多，但是我就没跟下来= = 后面再学就有点费劲…T T
网上有很多的共享的爬虫代码，各种都有，几乎你想爬的地方就已经有人给你爬过了..所以不会写的时候多搜一搜..只是提供思路，还有因为不了解很多关于HTML的basic知识可以通过别人的解释了解比如说网页的tag咋找之类的，但是网上很多代码都有很多错误..所以一定不要照搬下来，改错会改到崩溃的..别问我是怎么知道的-0-
还有就是，我觉得老师可以把GitHub早点告诉大家，里面真的非常非常有内容，不论是爬虫怎么写还是有些Package的介绍还有些小程序小项目都挺好的，我觉得如果一直看会对数据分析这些地方有很大的想法，也对Part1的选题有很大的帮助
2.Part1的选题：我当时就是觉得很艰难，因为感觉自己对数据分析没有很强的概念，所以不知道做什么，当时就去知乎看了有关Python的话题，看到有一个人也是做得相关的内容，就决定了做这个。后来知道了GitHub之后，发现了很多特别有趣的内容，觉得有点后悔但是已经开始做了那就..记得老师在总结Part1的时候也说了我们这个班做的题目都几乎是文本分析和公司啥的，比较集中。我觉得可能大家都会和我有一样的想法就是不知道该做什么吧，所以早点多看看其实真的比较开拓思维，也可以更了解Python可以做啥~
关于数据可视化，我觉得可视化最重要就是好看，虽然决定用Pygal，但是其他的我也大致了解了一下，以下是几个关于Python做可视化的库的我的用户体验：~
Matplotlib：这个老师上课也有讲，算是Python最常用最Basic的可视化途径了，一些简单的数理的统计又不追求美观，用它足够了。但是用它做出来的图真的很难看。。
Seaborn：它基于Matplotlib上开发的，所以自然语法上要简单很多，然后图形也美观很多。它的语句需要学过Matplotlib才可以了解一些默认参数的设置，而且它的图种类相对也算比较少。
Ggplot：如果你是用pandas处理数据形成Dataframe再做的话，就很方便了。他可以叠加图层，比如图加上线这样，但是据说它停止更新了，美观度也一般。
Bokeh：是可交互的动态图，在Notebook写可以生成很好看的图像。但我觉得语法有点难懂，不好写，相比较我更推荐Pygal。
Pygal：代码简单，很容易上手，而且做出的图像很漂亮。生成的是SVG格式，可交互图像，用默认浏览器打开是可以动的图，看起来就很高端~
还有就是，跳开如果不用Python的话，推荐几个好看又简单的网站！
大数据魔镜
Echarts
Highcharts
操作很简单而且很好看，图表类型有很多。值得一提的是Echarts地图类的可视化效果，真的很强大。
两者相比的话，我建议其实做可视化不一定要用Python，平心而论Python的可视化这个地方做的不是很好。这些网站首先free，而且功能也很强，省去了写代码很多操作就是拖拽，也很好上手，重点是真的很好看！！！所以也希望如果之后再有人做这样的Project的话，可以让他换个方向吧-0-
5.脸皮要厚，不会要问。
一定要去问问题，不管是课上老师讲的地方不会，还是课下作业，或者是自己做作业的时候不会都要去问，问Google问同学问老师问贴吧都可以，一定要及时问。如果我脸皮不厚就不会学会爬虫了，所以一定要问！
1.拖延症害死人。记得老师从布置就开始说一定要按照自己定的TimeLine去做，然后就一直觉得时间够用没有按，结果最后很赶的完成，还打扰了老师orz..幸亏老师善良，此处给老师比心！
2.感觉自己的选题有点简单，所以整个作业带给自己的成长不是很大。所以以后感觉要对自己高标准严要求，才能学会更多吧！
3.课程来说，总体觉得自己课下投入有点少，只是跟着老师上课的内容，没有自己课下去延伸更多，做完这个大大大作业之后才觉得Python有好多东西要去完成，要继续加油学习吧
4.最后最后


.总结：
①这次做这个最大的也是最开心的收获就是学会了爬文本，这样我以后看同人小说会方便
很多，想看同一个作者的所有文章也方便很多，想要保存某个作者的作品也不同大费周章
的慢慢复制粘贴了。我之前一直特别想学爬虫，因为真的感觉爬虫特别有用，对于以后专
业方向的进展也会很有帮助。然后就是做分析的时候特别有成就感，尤其是看着变量R^2
变化印证了自己之前的假设的时候。在该程序中尝试自己制作了几个简单的算法进行分析，
结果基本上可以解释，但并不是全部特别尽人意。个人认为是算法虽然有一定的意思，但
是对于想要描述的指标的精确度描述的还是不够。另外其实这里的文章爬取还是没能获得
全文，部分内容作者给了图链接还有其他网页链接，如果要完善，可以将图里的内容转化
出来。但是之后分析的时候要注意，有图链的章节本身就非常特殊，一般不会包含主题相
关关键词，但是对热度有非常大的影响，需要单独分析。用一个作者的长篇连载进行分析，
好处是基本保证文笔不变，但同时由于是长篇连载，读者黏性较强，对结果可能会造成一
定的偏差。但如果大范围的在tag 里所有相关文章进行上述的分析，又很难去度量文笔这样
一个虚无缥缈的变量，这是一个仍然比较纠结的地方。
②对后面同学的建议是真的注意时间的规划，尤其是课非常多的同学，一定要注意时间问
题，期末真的事情太多，如果后续发现想进行扩展也没有充裕的时间，或者要对课题方向
进行改变会非常匆忙。另外就是汉语文本分析和英文文本分析感觉差别很大，要提前意识
到这种差别并有预期，千万不要看了论文后才发现语言不同无法应用，前功尽弃的感觉真
的非常糟糕......所以我的建议主要就是：
一：千万注意自己最开始的研究主题的可行性及难度
二：注意时间规划


我通过这次作业复习回顾了很多以前学过的知识，也新了解了一些在课堂上没有讲过的知识。感觉对自己处理分析的数据的能力是一个综合考验，让自己的综合能力得到了一定程度的提高。我希望我在一开始就知道的事情就是，这件事情没有你想象的那么简单!虽然听起来很简单，就只是一个两根均线穿来穿去的策略而已，但当我实际操作的时候我才发现事实上各种地方，各种细节，各种你意想不到的地方都会出很多问题，包括绘制图像的时候。有一些地方是老师没讲过的，需要你自己检索信息，理解内容的能力的。很多时候一个细节上的问题就足以卡你很长的时间去找到并解决它。要完整的实现一个目的确实很需要精力，我这时也才体会到了要提高编程能力就要多动手这句话的含义了吧。在part1中我预估的时间完全没有实现。我在那时太高估我的编程能力，太低估完成一个项目的难度了。在原计划里我的最后一周是弹性时间，还可以用来补充更多更难的内容进来的。但实际上只实现已经规划好的任务就已经花费掉了我所有的时间。原因就是我太轻敌了。以后我想对想做类似项目的同学建议：一定不要轻敌！一定要以十二分的认真态度来对待你的任务，并且越早开始越好，因为它一定会比你想象中的更难！因为你会遇到很多完全在你计划之外的挫折。当然，编程老手的同学的话应该就不存在这种问题了。至少应该知道要十分认真对待了。总之，一定与预留出比你估计需要花的更多的时间来解决问题！另外，有困难的地方，最好先自己思考怎么解决，想不出来再去网上查找，最好是用google的英文查找，找不到再去问老师，问学长。


在这门课上学到了很多东西，做作业的时候也遇到很多困难。先是从思考论文写什么开始，不了解自己能做什么，会做什么，更关键的是怕做的东西数据找不到，数据不全，想了很多方法，看了很多文章，不断权衡创新性和可行性，也改了很多次题目，最后才确定这个看似抓数据可以自己做到，难度适中，基本能完成的任务。
做作业的时候也是出现很多问题，有时候就是往一个方向死命地想，找不到解决方法也不知道换一个方式。感触最深的是，python就像一门语言，用于交流的语言，想要表达出自己的东西方法很多，并不止是只有一种，一种方法行不通应该换一个方法去思考，从另一个角度解决问题。
数据的分析中会边分析边发现自己的方案的不足，并不断地去完善。先开始抓取的数据只有半年的，觉得没有代表性，在考虑抓取沪深300和上证50之间，最后选择了抓取更多时间上的上证50的数据。画图的方式不合适，又改了许多次。可能这个就是一个探索的过程，在这个过程中不断去完善，去发现。
很多东西别人说的自己没有亲身体验是不会感受到的。才开始布置作业的时候老师就说清理数据是最耗费时间的，当自己做起来才知道清理数据，检查数据，数据不够又要补充实多么费时费力。还有很多东西自己还是要学着自己去做，别人说的懂了但是没有体验过是不会明白的。
收获最大的是开始学着在网上查找代码，学习代码，查找函数用法。刚刚学习python的时候看到代码整个人都是头晕的，没有任何逻辑，掌握了一些知识之后能看懂代码，但最重要的是代码后面的逻辑，为什么是这样分析，为什么要这么分析，以后用到的时候那个逻辑就会成为自己的思维。


首先，我一定要发自内心的感叹一句：“我终于完成持续六周的python期末大作业了,哈哈哈哈！”大学至今为止，我还从未如此认真、如此完备、如此兴奋的完成过这样一个工作量非常庞大的期末大作业，我甚至和一起选这门课的同学都互相调侃，说python让我们把一门公选课学的比专业课还专业课！
回顾整个研究的心路历程，最开始选择研究《冰与火之歌》的主角分析还是因为一篇微信推送，那篇推送上面大致的介绍了社交网络分析的各种方法和理论，正好和老师介绍的理论不谋而合。加之我和小伙伴们都特别喜欢看《权力的游戏》，因此就想到把两者结合起来，于是这个题目就诞生了。而当我在查阅文献时，发现国外居然有学者做过类似的主角分析，并且所用的方法也是“NetworkScience”，这时我便知道我的研究是可行的。决定了做这个研究之后，就开始大面积的填补知识，包括图论的基本知识、网络科学的基本概念、爬虫（因为要获取人物名称数据和原文数据）、数据清理需要的正则表达式、python做网络分析的Networkx包，并且还复习了这学期所学的Pandas、画图等相关的内容。这个过程不得不说是一个非常痛苦的过程，因为学习这些比较前沿的知识都需要查阅英文文献，这就对学习造成了巨大的障碍。加之很多东西我没有基础（比如我这学期才上统计学），所以很多知识难以理解。在这个过程中，有好几次都抱有“放弃吧，水一水”的想法。最终我为什么坚持了下来我至今都不太清楚，但有一点是可以肯定的，那就是我的好奇心需要得到满足——因为我确实想要知道谁是《权游》主角哈哈！
学习完基础知识后，剩下的就是写代码来实现。这个过程遇到的障碍也不比学习时候少。最开始是写一句代码，报一句错。每次运行程序时都像在买彩票一样，期待能中特等奖（程序运行不报错）。在熟悉了一周语法后，bug倒是少了许多，但在算法上的问题又不知道该如何解决。因此，这时候我就广泛的向计科的同学求助，在求助的过程中我发现计算机科班出身的同学在思路算法上确实要比我们非科班的有条理许多，因此，在这个过程中，我也收获了不少（当然也付出了请他们的吃饭的饭钱哈哈）。当所有程序运行出来，结果弹出来的那一刻，我内心的激动已经难以用语言来形容了，或许这就是成就感吧哈哈！
当然，在我的这项研究中还有非常非常多的不足。首先，龙妈没有出现在任何一个指标的top10，这个结果非常不符合主观认识（毕竟很多人都认为冰火有三大主角：什么都不知道的JonSnow，小矮子小恶魔和有很多称号的龙妈），这点在正文中已经提及过了。除了龙妈这个bug之外，对于只用人物的名称来研究的这种方法，还可能产生其他问题。比如，文中可能出现“LadyStark”来指代史达克家族的大小姐三傻，但“LadyStark”并不一定指代的是Sansa，它也可能指代Arya，因此就不能简单的把一个称号对应一个人物，这就对研究造成了巨大的困难。我查阅了许多资料来解决这个问题，但至今为止也未能发现特别好的解决方法。

要说这门课或者这个研究带给了我什么收获，我简直要说收获那真是太多太多了！首先，作为统计专业的同学，编程是必备技能。但统计学院只会开设R语言、SAS等传统的统计软件编程课，对于python这种在大数据挖掘、机器学习方面很有优势的课程就很少开始。因此，这门课是对我专业知识的一个补充。其次，这门课把我引进了数据分析的大门。我原本的职业规划就是做金融数据分析相关的职业，但在上这门课之前，我还从未真正的接触过数据分析，不知道该从如何学起。上了这门课后我才发现数据的世界原来如此精彩，更加坚定了我学习数据分析的决心（然后这几天发现电子科技大学开了一个新专业叫数据科学与大数据技术，感觉自己被抢了饭碗）。
最后我想说的是，我非常喜欢刘老师的上课方式，老师在上课时只是给大家一个introduction和一个知识的框架，给大家一个学习的方向，然后同学们需要根据自己的需求和兴趣去自学和延伸。这种方法使我们不局限于课堂中的内容，还提高了我们的自学能力（感觉研究期间读的英文paper比我大学两年加起来还要多），对将来的学习和研究肯定非常有帮助


其实，这学期以来一直挣扎在退课和继续学的边缘，一方面觉得学python，做project很麻烦，一方面又想提升自己的编程水平。从第一次作业利用tushare包编写程序、到两次期中考试、再到期末论文手机评论文本分析，每一次任务做的过程都非常痛苦，让我一度懊悔上学期期末没把这门课退。
但确实，每一次任务的完成都带给我无比的愉悦与成就感，因为这是一个需要自己学习研究各种package用法、反思问题解决报错、一次又一次调试程序精益求精的过程，整个过程都需要自己独立完成，而这都需要你的静心、耐心和恒心去支撑。在做期末project的过程中，自己无数次的觉得做不下去了想砸电脑、无数次在程序报错之后抓耳挠腮，无数次的遇到瓶颈，但庆幸自己一直没放弃。在遇到了大大小小的问题后，只要多坚持一下，就是柳暗花明又一村，这种快乐只有经历过的人才懂，只可意会不可言传。一切的困难在解决之后再回头看，似乎都变得简单而有趣，才真正的领悟了苏子笔下的“回首向来萧瑟处，也无风雨也无晴”。
还有一些在编程编不下去的时候胡思乱想，想到的一些值得分享的感受：
1. 要想真正学好编程，就应该从做大大小小的project开始！！四个月前自己还只是旁听了一个学期C++编程刚入门的小白，虽然现在还是很菜，但至少能真正利用编程去解决实际问题。在自己独立做project，解决实际问题的过程中，让我真正感受到了编程的魅力，甚至后悔当初怎么入了金融的坑没去做个沉浸再自己世界里的快乐的码农……自己不是按预期计划的每天抽一点时间做完的，而是抽了几天时间只做这一件事，精力集中做完的任务，虽然这几天过的很糙，每天穿着拖鞋、随手抄起一件T恤套上、顶着蓬松的乱发，一副宅男形象就出门码程序了，不过这种专注的快感确实让人回味无穷！！

由于python是开源的，并且语法很灵活，因而同一个问题，你可能在网上能够找到n种解法，然而你能找到的代码直接复制粘贴过来99%都会直接报错。因为网上的程序可能是在python2环境下编写的，很多语法规则和python3下会报错，或是存在其它问题。剩下的1%就算不报错，但跑出的结果不是你想要的，无法解决你自己的问题。所以，不要想着走捷径，唯一的办法就是静下心来去学习每一个package、每一个函数、每一个参数，做到能看懂别人的代码，理解其精髓，才能修改借鉴为自己所用。不过倒是发现一个窍门，如果报错了把错误提醒直接复制粘贴到百度或谷歌，倒是多半能较快发现问题、解决问题。
3. 最重要的事情说三遍：基础很重要！基础很重要！基础很重要！课堂上学tuple、list、dictionary的时候觉得挺简单的，也没太上心，实践的时候才发现真的是失之毫厘，谬之千里……可能遇到一个报错令你百思不得其解，改来改去都没用，最后发现其实是type不对，改个类型就好了。因此，熟练掌握每种数据类型的特点，相关的函数应用，真的非常重要，能起到事半功倍的奇效。自己在做project的时候还恶补了很多之前学过但遗忘了的基础，因此也没少走弯路。


这次项目可能是我本学期完成的最大的一个作业了，想想三个月前我还觉得爬虫是一件我不敢想的技能，现在也算有所收获。其实听到我们这门课最后的final work以后想了很久，什么才是有意义又有意思的，刚开始想的是验证“口红效应”是否正确，但是由于10年以上多个国家的廉价奢侈品数据实在是太难获得，所以最后非常不情愿的放弃了。选择廉价机票飞行策略的原因是由于个人爱好旅9 
行，编出来的东西今后也对我管用，并且我以为网上的案例应该非常多，但是事实证明网上的东西就算复制粘贴也会出现各种问题，并且鱼龙混杂根本无法得到自己真正想要的参考资料。前前后后可能想题目花了一两周，编程花了半个月才完成。
好像对于我来说，没有最困难的部分，因为都很困难。在编写仅仅一个城市的爬虫代码时，显示了很久<连接没有反应>问题，前前后后试了两天才成功。再爬去全国所有城市信息时，由于该网页不是通过翻页获取信息，所以我是手动复制粘贴cheapflight信息的，前期准备时，没什么事大概每天复制粘贴50来个城市的信息。数据分析部分主要是检索匹配有些绕，花了一天一鼓作气，从早到晚最后匹配出来了，最后我抱着电脑回寝室，第一件事就是问同学，你有什么想去的地方吗？我帮你查机票信息，给你做个策略。
心得： 1、最好给自己留够足够的时间去思考，得到清晰的思路后再下手。不然就算有参考也会耗费大量时间在尝试上。除此之外，多留些时间还可以满足自己的新想法，就像我前文说的还可以将信息进一步整合，给自己留有更大的改进空间。 2、用最适合自己的办法。我在网上找了些信息匹配检索的实例，大多都是用函数来解决问题的，操作比我现在做的要简练一些。但由于能力原因，我个人无法很好理解别人的方法，所以最后用了自己的方法来操作。 3、做自己感兴趣的东西，编程序本来就很不容易了如果还做的只是一些看起来高大上但是自己不怎么了解、不怎么感兴趣的东西，那就是折磨人了，试着从自己喜欢的事里挖掘一些有意义的东西来做。


这一部分，应该是我这篇报告里的核心部分了。通过这次作业，我有很多的心得和体会。从一开始的选题到最后写报告，这是一个学期的完结也是一个收获的过程。
从我自身来说，我本身对于编写程序的基础就很差，而且对于程序编写的学习往往会不如多数同学那样能够很快理解也能有很好的想法去通过程序来实现一个问题的解决。我也充分认识到自己的缺点，一开始学起来时不时会冒出放弃的想法，会想要放弃，会找个接口来不断的拖延自己的作业，不断的找借口来逃避主动学习。但是，我们往往实在有外界压力的时候，就会主动去学习了。
最开始选择主题的时候，我有了解很多利用python作为分析工具来进行一些研究的案例。那段时间，我最担心的就是自己的python作业，我到底要做什么，到底应该如何去完成这个作业。后来，我选择了我现在的这个主题，其实，我的这个小小的主题也不算什么案例的分析，更谈不上研究分析。我运用到的大多数工具包都是这个学期所学到的，或者提到的，与其说是一份报告，还不如说是一个对本学期所学内容的回顾。我的数据来源很便捷，没有像很多同学那样通过一段爬虫来获取数据。我获得的数据是简单明了的，对于数据的分析，我采用的也是绘图软件。每一个阶段的内容，在我看来，和很多同学比起来，根本就是最基础的。但是，我觉得简单不代表没有价值。虽然我爬取的数据很少，但是，对我自己来说，这是一个进步，我在学着做云图的时候，也老是出现各种问题，我不断的“骚扰”我学计科的同学，我觉得他们都快被我烦死了，不过，后来能够画出来，我还是有了小小的成就感，我会在作业文件夹里，把最开始的图片放上去。
在完成作业的过程中，我也时常报错，每一次小小的错误，可能不那么容易被察觉，我会不断的去检索错误的原因，不断的去改变方法。每一次报错，内心会很沮丧，但是，也正是因为报错，我在这个过程中学到了很多平时忽略的点，在这个过程中，当通过一次一次的检查改正，最终有一次实现了的时候，那种快乐是发自内心的。
通过这次作业，我重新认识了学习程序带来的快乐，最大的收获便是让我对于python产生了兴趣。学习程序，最重要的能力是检索发现和创新。学习的过程中，或许小小的变量或者格式都是影响顺利运行的关键。那么在学习的时候，要善于理解以及实际操作，只看程序，而不去实际动手，不会有任何的收获，所以在检索学习的同时要不断实际操作。在这个基础之上，需要学会用简短有效的方式实现目的，初学者往往只会按照原始的方法去解决问题，然而灵活的运用更加有效的方法也是十分重要的。当然这一个层面需要拥有扎实的基础以及自我检索能力。最后，还要不断的想身边的同学学习，不断的去“骚扰”他们，这样你也能够学到很多东西。
这些就是这次作业带来的心得体会，我的兴趣也正是因为这次作业而产生，对我而言，这个小小的程序却是意义非凡。


第一次一个人完成工程量如此大的项目作业。走完一个数据分析的整个流程，中途感觉
遇到各种bug 和问题。仅仅是爬数据方面就首先遇到了美团的ajax 交互式创建网页，那时
候自己只了解一点点爬虫，完全不知道怎么办，这里要特别感谢余助教专门花了一个多小时
帮我解决了这个问题。以下是我的一些体会：
1.现实中的数据真的相当复杂。不仅表现在数据量和数据维度上。在爬取数据过程中，
我遇到各种数据缺失，数据错误，甚至有时候数据本身的真实性都值得怀疑。例如在数据试
探过程中发现“味道评分”、“服务评分”对外卖销量没有影响，这显然是违背常识的，而
我去查阅相关的论文发现已有的论文已经证实他们存在相关关系。我考虑最大的可能是数据
失真，即味道评分和服务评分没有真实地反映商家菜品的味道和服务。
2.数据分析是一个繁琐的过程。一旦某个地方出了问题，后面所做的大量工作可能是竹
篮打水一场空。我刚开始爬取的是美团数据，进行了初步建模分析后发现自己的数据存在很
多问题，首先是数据量过少，考虑的影响因素过少，模型可信度差。后来，我又重新从饿了
吗获取数据，增加了数据量以及变量，模型解释度得到了一些优化。
3.“饿了吗”真是一个很好的外卖平台。强大的反爬虫机制，在爬虫过程中应该是我遇
到的最头疼的事了。
4.编写代码时程序应该有全面考虑各种可能的情况，这个问题我在写爬虫的时候遇到的
频率最高。因为爬取数据时经常会遇到数据是空值，不同网页获取的数据类型不一致的情况，
自己写的程序因此报错。（try-except 是解决这类问题的好工具）
5.做一个调包侠心里真的很难受。以后要好好学习统计和机器学习的知识，希望自己也
能开发出一些牛逼的算法和第三方包，为开源社区做贡献。
6.最后，再次感谢老师对我项目所遇到问题的耐心解答，感谢余助教专门花了一个多小
时帮我解决一个爬虫问题。项目作业虽然得到的结果并不太好，但是自己在其中还是感觉很
有收获，坚定了自己在python 路上继续走下去的信心，做一个python 的铁杆粉！



我觉得最大的感受是要在选题前先了解一些数据分析的知识。因为我对于数据分析方面的内容之前基本没有接触过，在最开始选题的时候就很犹豫，很多论文写的内容也不理解，没有办法模仿，最后只能从课堂上老师讲的一些内容下手做分析，所以选题和研究都很受限制，没有真正体会到“数据分析”的魅力，有一点小遗憾。
在时间安排上，基本是比我预定的时间晚了一周多。除了因为临近期末事情多耽搁了几天之外，最重要的原因还是自己把数据获取想的太简单了，尤其是数据获取后的清理过程，几乎花了我大部分的时间。写爬虫的时候，可能查看一两个网页，它们的格式都是一样的，但是当要获取几千条数据的时候，各种问题都涌上来了。有些可能网页根本没有数据，有些可能数据不是最新的，有些可能会在网页里插入一些其他说明，等等，这些都需要对不同情况做特定的处理。有些甚至是我到后面进行分析的时候才发现，比如本来我需要抓四年的数据，可是获取的数据只有两年，又不知道究竟是哪两年的数据，所以只能重新修改抓取的程序，再跑一遍获取新的数据。这样来来回回折腾了好多次，也花了很多的时间。
最大是收获是学会了思考如何让自己的程序运行效率更高，代码更简洁。Python强大的一个地方就是它有很多自带的函数，相比较自己写出一样功能的函数，自带函数的运行速度快很多。特别是在DataFrame中，尽量使用DataFrame的函数实现操作，这样比自己写循环的速度要快很多很多倍，我在后期修改程序的时候才体会到这点。而且大部分的操作是能够通过网上查到相关的函数的，当然有些操作我的确没有办法准确描述，也没有找到合适的函数代替，所以还是强行写了循环。DataFrame还是有很多功能值得探索的！



其实在这个学期的python学习中我是感受到非常的不适应的，因为这是一门非常非常非常简单的方便的语言。在大一时我在程序设计课中选了c++，当时的感受和现在在用python时完全不同。初学c++时我是感觉自己处在一个原始世界，从使用最简单的工具做着最初级的事情；到学会了使用更复杂的工具，可以把原来的工作做得更加一般化更加通用；也学会了怎么把原来复杂的工作依靠复杂而危险的方法，比如指针和引用，而变得更加干净利落；再到更加高级的类、派生和遗传，工具虽然变得越来越复杂，但是是一脉相承，能干的事情越来越多，也可以越来越一般化通用化。回顾那个年轻的时候，我发现我碰到的问题都是前人已经造过的轮子，我思考的问题也是怎么从我的方法中选出那种最合适的，造出一个最最好用的轮子，当时我觉得这是一个特别特别好玩的事情。但是后来我知道了，当时的错觉是因为我离解决实际问题差的还太远。
所以在面对着python的时候我起初是迷茫的，尤其是自己用到最基础的工具写的算法，python里面都是有函数的，而且函数用起来还比自己用python语言写的快，以为可能函数里面是用c或者c++写的。而当使用python时应该做的就是，先猜一下你要实现的多大范围的功能会出现在一个模块或者一个函数中，然后把它学会了再调用出来，这是一个查字典的编程语言。虽然他带给我些微的惆怅，在使用的时候我往往都不知道为什么，但是他就是把结果给我拿出来了，令人非常非常的崩溃，但是我知道，可能这样做事情是正确的。
所以其实在这次的作业中，我留给数据分析的部分非常少，和我在part1预料的完全不一样，因为我确实还是对查了很多资料最后写了一行import再写一行调用函数这种事比较烦，不过在网络数据采集的部分我还是找到了很多的乐趣的，因为这个环节很大程度上是没法统一用一个包或者一个函数实现的，比如对于url是否有简单的规律，怎么解析网页提取信息，是需要基于具体的情况进行分析，也使我在python中还是找到了一些自己的存在感。不过另一方面，python即便在这网络数据采集上还是具有非常强大的功能，它已经使这项工作变得相教从前而言极其的容易了。
但是在此我还是觉得python是有缺点的，不在于它本身而是在他的使用者。因为python会使简单的工作变得极其简单，所以当一个需要更加复杂的工具时，造一个新轮子的工作就会尤其痛苦，这是本来我想要做的，也是我没能做成的。我其实是很想做成的，但是已经过了deadline还是要先交作业，姑且留下这些感受，希望后人能以之为鉴，不要步我的后尘。



不足之处：
没能找到更优的语料库。
没能做到更精确的分类器。
本来还想通过死囚的个人信息进行分析，没想到绝大部分都是以图片形式进行保存，如果有知识进行图片文字识别应该能做得更棒。
学到了什么：
（1）Pandas，学得最多也用的最多的
（2）爬虫，上手有些难度，不过学习速度也越来越快了
（3）自然语言处理NLP，这个学得最久了，先是买了书学NLTK，发现不够用，又学了点机器学习
更多的是精神的收获，证明了自己，无悔选两次Python课，每一次都用了跟必修课当量的时间在学，可惜如果不是学业问题，如果我是大三或者大四，我可以写更多更复杂的代码，把这个做得更完美，不过看我的程序走到这一步，82%的精确度我已经很满意了。学NLP，从图书馆开门敲代码敲到了关门，只吃了一袋面包，走出图书馆感官体验矛盾，饥饿、充实。 
我写的代码可能很简朴，不够优雅，但是用我的双手创造的，孤军奋战，没问过任何一个人，也没从网上copy过代码。 
小学曾跟别人说未来自己要当黑客，至少现在我可以告诉他我有能力能成为一个黑客。 
我决定下学期学习数学建模，量化投资，继续前行。 
希望一开始就知道的东西：
没什么，基本上算从零起步，所有知识都是一点一点翻书和搜索到的，代码从学到的灵感中得到的，一行行敲出来。
给做这类项目同学的建议：
语料库要找到更好的，我选的语料库我觉得还是不算太好，没办法。最好是能用非监督式学习做这个项目，不过我水平和时间有限，做不来了。
这个项目可以控制的更精确，如果有渠道找到更多的数据会更好。
还有我在数据集中说的两个与本报告分析无关的结果，可以的话可以继续做下去。


本次课程论文对于我来说，其实是并不成功的。虽然一开始对这个问题有很大的兴趣，但是通过老师的提醒以及实际操作，就会发现，我在问题的设置上存在很大的难度。找了很久相关的文献，都没有找到特别类似的可以拿来做参考，确实感觉压力很大。准备前期，就是搜集数据，最开始我研究爬虫研究了一个月，但是确实由于电脑软件出了很多问题，加上自己理解不够，无法把原目标庞大的数据爬下来。最后几天，我选择借助数据软件的力量，但是得到的数据量也十分有限。
对于这次课程作业，我的心得如下：
（1） 研究过程最关键的就是对问题有清晰地认识和思考，要知道每一个问题的关键点和努力方向。因为我在设置问题上的问题，所以我遇到了很多困难，以我的知识并不能解决。

（2） 平时还是要多注意积累，很多代码交流的网站论坛，还有相关论文其实应该都多了解一些。这样用的时候才不会手忙脚乱。在搜索文本上浪费了很久的时间。

（3） 考虑数据集的时候一定要确认好来源是否准确。这次我把数据集放在了豆瓣帖子的发帖人内容上，结果得到的大部分数据都与口红本身信息无关。直接导致问题结果不准确。通过分析文本我才发现，其实帖子的回复信息才是包含了大量有用信息。然而时间和操作过程并不允许我重新做一遍了。还有就是我没有考虑到博主重复发帖刷存在感这种事情，特别是微商代言三无产品，会频繁发帖，直接导致数据量出现了偏差，造成了结果非常不准确。

对Python要有足够的耐心和学习精神才能够成功，很多时候就是因为太急躁就做不出来，慢慢练习，在不断的错误中才能积累到知识。量变之后就会发生质变。
这次课程论文实践，我犯了很多错误，做到最后得到了非常错误的结果。可以算是一个反面典型了，我觉得问题主要出在一开始的问题设置上，我对问题的设置和我想的解决方法是不吻合的，这就直接导致我得不到正确的结果。对可能影响误差的情况没有及时预料到，也是一个惨痛教训，希望同学们以此为戒。

一定要坚持到最后！这门课虽然辛苦，任务量大，但相比起收获的东西实在是太微不足道了。自己现在仍坚信一个想法，如果自己还有机会，一定会再上一次。若要说有什么具体的建议，那便是下来一定要结合老师上课所讲的内容反复练习，不懂的地方查资料一定会有，这样不仅上课的效率高，而且下来也能完成更多你以前所不能像的东西。就像自己在学习完老师所讲的爬虫后，又回去一口气看了50集的爬虫教程加以巩固，直接为之后计量及相关课程的作业打下了基础，最终获得了前所未有的评价。还是那句话，只要能吃下这一时的苦，什么就都会了。


真的是花了很多的时间在这份作业上，也可能是因为基础较差，所以很多资料中给出的代码都看不懂，很容易看着看着就烦了，有一段时间几乎是整天整天在查阅资料、解决问题上，不知不觉就过了很长时间，写程序是一件成果导向性和及时反馈度很高的事情，挫败感和成就感几乎同时存在，不断激励着我继续写下去。
在这过程中最大的感慨就是，已知的困难都不算困难，真正会感到焦虑和手足无措的是种种未知的情况的发生，不论是各种花样百出的报错，还是发现得出的数据分析显示的结果和想象中的完全不一样，都要学着接受（微笑脸）。
而且在面对某一个问题时，重要的不是忙着去找解决方案，而是对问题本身的分析，抽丝剥茧似的提出假设再去验证直到找出最关键的地方。同时，在满足基本要求后，便会不自觉产生更高的期望，对数据拟合度的优化，对图表呈现形式的改进等等，总之还有很多需要学习的地方。


1、量力而行，有一定编程基础再学会更好，否则需要花很多时间理解程序的逻辑。最后做项目的时候很需要去根据python语言的逻辑尝试编程方法，如果只是记住课堂讲的内容就很难想到更适用的写法。前期学习基础的时候多思考python的语言逻辑，结合网课和老师推荐的资源去练习理解。
2、需要像学数学一样持续固定地划出时间来练习代码，时间隔久了再写会卡很久。Python语言也算是一分耕耘一分收获，对于程序悟性不佳的同学更是需要多花时间理解去掌握。
3、本课程更多以论文研究为应用范围，集中讲数据分析，不适合想要学习爬虫的工作党。但课程讲解关于python的语言逻辑，也会花部分时间简单介绍爬虫的实现原理，对于爬虫入门有帮助。
4、术业有专攻，如果没有决心投入时间精力去学的话还是不要任性选课。


这门课是很值得选的但是要估量一下自己的实际情况 要有兴趣也坚持 第一 节课老师是很真诚地 节课老师是很真诚地 节课老师是很真诚地 节课老师是很真诚地 告诉大家实情 告诉大家实情 告诉大家实情 ！！！不过努力也是可以学很好的 因为是从零开始 只要 有一些 编程方面的思想也行 编程方面的思想也行 编程方面的思想也行 编程方面的思想也行 编程方面的思想也行 。另外 。，我发现很多的旁听同学还是坚持下来了 我发现很多的旁听同学还是坚持下来了 我发现很多的旁听同学还是坚持下来了 ，很厉害 。
2. 平时要注重练习 提高熟练度 。老师布置的课后 老师布置的课后 老师布置的课后 老师布置的课后 阅读和 阅读和 作业 都要按时去做 ，因为量还是很合理的 推荐的书目可读性也很强 。
3. 善用搜索用英文 +Google搜索 ，注意关键词的选择 ，注意关键词的选择 ， ，基本上可以获得想要的东西 ，基本上可以获得想要的东西 基本上可以获得想要的东西 。 很多时候先自己去加搜索和尝试 不能解决时再问老师和同学可更好 ~
4. 注意时间规划勇于尝试 。期末作业早做打算 不要抱怨 ，多干 实事 。（因为即使最后仍然不能解决但肯定有其他的收获 ，但肯定有其他的收获 ，比如我学会了用Eviews做系统GMM……）

其实就这篇论文来讲，没有最困难的地方，因为觉得处处都不轻松。理论上找了计量经济学的老师，结果因为领域有偏差，老师并不十分清楚有关时间序列和隐马尔可夫的相关知识，急得自己只能一连三天蹲在机房整天的研究，借书，查资料完全是家常便饭。模型在python中的实践更是十分困难，由于隐马尔可夫被python3.0及以后的版本废弃了！！废弃了……所以自己只能在网上查找各种时间方法，当时资料查找的真的是自己都快编出一个代码集了，最后终于找到了相关较为详细的教程，才将隐马尔可夫这个难关攻下来，那时真的感觉整个世界都是我自己的(=.=)。到了后期数据的处理也是十分的复杂，几乎是私下里爬虫的数倍任务量，越做感觉就越多，也只能什么都不想的一点点的熬夜抽时间做完，最后还是感觉挺有成就感的。


终于，历经近三周的时间将final project做完了。做的时候似乎有很多可以吐槽的，但似乎从来没有产生过放弃的念头，其中在数据清理的那一周，连续几天做梦都是在编代码，早上醒过来，赶紧翻下床试一下梦里想出来的语句适不适合处理数据。现在却只剩下释然，虽然还是有很多不足的地方，但至少对于自己来讲，之前没有过任何编程基础，最后拼了全力完成了一行行的代码，还是很欣慰的。
回顾整个Project的历程，看着写出来的代码，心中有些疑惑明明做了很多，明明试了几百次，整合出来看起来很容易的样子，可能是那些错误被改正后，选取的往往是最直接最明显的方法。怪不得看论文的时候总是看到作者轻描淡写方法，现在才明白他们在光彩结果背后付出的努力，往往是基于一次次的trail and error。
整个过程中最难的是数据挖掘和清理的过程。天气的数据属于一个庞大的数据库，气象站千差万别并且难以整合，网页中的所有办法都尝试过，遇到不会的就去查，尝试写出代码看能不能得到想要的数据，连续三四天都没有任何进展，想来想去，尝试在网上找到原作者的E-mail地址，凌晨的时候发出了邮件，可能刚好是作者的工作时间，看到邮件后就很快回复我，告诉我他的数据是买的CD。看到作者的回复说时候有点激动，本来没有报什么希望，最后竟然会得到作者的回复，还是学术成就很高的学者，这可能就是大家经常说的学者风范吧。这让我在抓取数据疯狂的时刻，冷静下来，采用了网页下载的方式，获得国家的天气数据。本以为股票的数据会较为容易一些，后来发现文中Datastream的数据库需要有认证才可以进入，申请了学校图书馆的Wharton数据库，虽然包含了Datastream但是目前并没有购买其使用权。最终只能使用CEIC的股票市场数据代替。

在数据清理方面，主要用到pandas来对Dataframe处理，原始数据较为混乱格式不规范，需要重新写入文件，将日期形式转为时间戳，求得每个工作日规定时间内的平均云层覆盖率。期间遇到不知道怎么分割转化为时间戳，分割年月日小时，求当天的平均值等问题，都是现在Google或者Bing上面搜索关键词，后来发现往往得不到自己想要的结果，于是开始自己试着写，先用一个国家做，再编写函数，将剩下的所有国家进行处理。本来以为自己会写不出来，后来发现尝试一下，得到的结果比网上一点点搜的效率高很多。在方法上，由于对Logit函数了解不够透彻，又重新补了一下这方面的知识。
和计划相比，是我低估了数据挖掘的难度，我把数据想的过于理想化，因为在文中数据挖掘不是重点，作者没有详细提，就理所当然地认为按图索骥就可以获得想要的数据，实施起来才发现被限制了。和计划相比，真正开始做Project的时间延后了，虽然前期看了不少资料，但是在脑海中想象怎么做和实际做的时候差异还是挺大的。计划只是在对未来未知时候的美好期望，只有早点实施，才能不断调整计划付诸于实践中。
在紧张的期末复习阶段，朋友得知我每天白天复习，晚上写代码到凌晨两三点，问我你后不后悔选这门课，我说没有，我还是很喜欢这么课。回想开学的时候，刘凌老师的这门课爆满，只有31个容量，连续刷了好几天，在系统关闭前选上了这么课。每周周一的课最多，连续九节课，到晚上的时候很累，但是一上课就精神很好，很享受老师讲课并尝试写出代码的过程。经管学院没有开过电脑课程也没有要求学习大学生电脑基础，所以对于我来说，编程是一个全新的领域，我很崇拜程序员，觉得编程是一个遥不可及的事物，同时也知道和其他同学相比有很多知识点的空白，幸运的是遇到了刘凌女神，一直鼓励我们问问题，每周我都会空出一个大的时间段做一些简单的代码练习，渐渐发现，好像并没有我想象的那么难。
整个学期的每节课对我来说都是一个巨大的信息流，一些介绍的内容虽然还是不大了解，但是让我对一些领域了初步的认识。这门课程还教给了我许多有关科研方面的知识，算是让自己变得更加professional一些。很清楚地记得最后一节课刘凌女神说到的博士就是为了更好地改变这个世界，即使是一点点，这让我坚定自己将科研这条路走下去的新年。
这学期又飞快地结束了，但是对Python的喜爱好像又加深了，虽然我不知道自己的能力可不可以继续向大数据挖掘等方面拓展，但是还是愿意尝试一下，就像刘凌女神鼓励我们做project，即使难也要坚持下去，多坚持一下会看到不同的结果。
最后，很感谢刘凌女神，也感谢自己，在这一学期坚持了下来，激发了自己对于大数据的兴趣，也坚定了之后利用Python做出更好项目的决心！
在现在大数据背景下，金融领域也在追求Fin-tech，Python在最近一段时间更是一个被神话的对象，朋友圈里也是经常出现“零基础学习Python课程”等推送。在选这门课之前，一定要想清楚，到底是喜欢这门课还是为了跟风，如果是后者，最好还是不要选了，因为之后可能发现痛苦地坚持了下来却也厌恶上了这门课。就身边现象来看，上课的人也从最开始的教室没有位置到最后的剩下选课的同学，做Project最困难的时候还有人在QQ群里匿名说没想到一个选修都这么难，开始自怨自艾，埋怨老师给分低，如果要是为了得到选修课的两分想要混这门课的，最好也不要选了，因为刘凌女神不会让你轻松拿到高分的。
在想清楚自己需求，进入这门课程的时候，就需要全身心地投入到她。编程技术的提高需要练习和精力，孰能生巧，一门语言多练了结果自然是好的，不要害怕遇到错误。我几乎没有一次就过的代码（可能是自己技术太渣了），刚开始遇到错误很慌不知道如何解决，就一个个搜索明白其中的意思，以至于后来写代码一次成功我都怀疑是不是某个地方用错package了。选Project的时候，一定要先看清数据是否可以获得，作者对数据获取方法只描述了一两句不等于他们很简单地获得。写代码的时候，给自己一点信心，先试着按自己的想法写，在某一个地方不知道用何种语句的时候再Google或者Bing一下效率会更高。多问问题，没有“stupid”的问题，更不要担心别人嘲笑你；多点开女神在PPT中的链接，会发现很多“新大陆”；多站在计算机的角度思考编写运行代码……
最后的最后，送给所有选课的同学：只要凭借着一股“Python虐我千百遍，我待Python如初恋”的热情，一定会爱上Python，爱上这门课的！


因为我是直接写的本科毕业论文，所以就直接用了学院的选题，是一个财务管理方面的实证。之前学院催初稿的时间很早，急着赶了一版就用stata直接做了（对不起老师我还是用了stata...），然后想着后面再用Python做一遍就好了，所以只给最后写代码留了四五天的时间，然后就被无情打脸了。也有平时基础不扎实的原因，在ddl前一天还去刷了个夜才写完。

本来的安排是，一天写完数据清洗，一天写完描述性统计，一天写完建模及分析，可最后事实是三天写完数据清洗一个通宵写完后面两个部分以及对代码简化优化。所以对和我一样基础不太好的人来说一定要留出充足的时间，因为写代码总是会不断遇见新的问题不断遇见新的问题，然后一个一个地去解决掉或者看能不能绕开。

最开始做数据清洗的时候，我直接用了Excel的思维去做，后来在经过Python不断警告和报错之后发现走了很多弯路，比如根据两个index进行dataframe的合并，在Excel中要建立一个辅助列去vlookup，pandas中直接merge然后传入两个index的参数就好了。另外还发现Python做完数据清洗后的可用数据比直接Excel筛选要多，所以结果有略微的不一样，会更加准确一些。以及在最后写传常数项以及面板回归的方法的时候面临崩溃，遇到的问题比如常数项不能直接往dataframe的解释变量列中传，以及双解释变量、平方项解释变量该怎么传常数项，在stackoverflow上大概看了30个回答之后终于写出了方法。

回头开始写summary的时候，突然又觉得好像花了很大力气也没写什么东西，也没什么难度，也不是很长，好像就和平时的作业长度差不多长...所以在选题上还是建议多考虑一下可行性和难度两方面吧。

最开始选课的原因是听同学推荐的，虽然在第一节课老师的恐吓之后也有点被吓到...因为对于Python的了解只有一点入门的知识，也没有做过相关的项目和训练，但是想到用一门具有挑战度一点的课来结束本科的课程也没什么不好的。后来发现也没有那么难，虽然数据清理那次期中考试一半的题目都没做完...

老师在课堂上有一句口头禅大概是“这个挺好玩的，大家有兴趣可以自己试试”，最开始完全get不到这些东西有什么好玩的，大概学了半学期之后慢慢体会到了Python在处理问题上的灵活性，以及有时候看到一些代码会发出“卧槽这也可以”的声音。最后写完大作业之后，对Python的理解还是深了很多吧。

课程的框架上，老师覆盖面已经很全面了，但我有种感觉对于Python基础的部分还可以再减少哈哈哈，因为后面的内容真的不够听，希望可以把机器学习那部分的内容再多讲一点以及做个相关的作业。整一学期的课听下来从编程小白变得还是能用Python做一些工作了，另外对Python增加了很多理解，统计方面的处理Python基本和R语言差不多，二者代码也很像，但Python对于文本的处理，网络信息的抓取，机器学习等处理有很大的优势。用到最后有一种相见恨晚的感觉...也希望以后能继续学习这方面的东西，把这个工具用好。

以及我的本科最后一门课结束啦，收获很多，很开心，对未来要修读的同学的建议就是对这方面感兴趣的话就放心地选吧，会学到很多东西的，但是也要准备接受打击哈哈哈，班上的大神同学真的很多。以及老师和助教都特别好，能给到在这方面最好的建议和帮助。大四老人准备下架了，祝大家享受你们的本科时光和Python for Data Science~

Q1:本次作业学到了什么？
这是我第一次使用 Python 语言完成比较完整的论文撰写工作，也是第一次用 Python 做计量研究。面对新的工具，在使用起来还是有些生疏，需要预留大量的时间去检索和学习相关方法，所有首先，通过这次 Final Project，锻炼了我的信息检索能力，养成了不懂就查的习惯；
其次，在计量中会面临不少重复性的操作，创建函数或使用循环等结构能大大简化工作量，但是一开始写代码对写循环、写函数很不习惯，通过后半学期的练习，开始有简化代码的意识（虽然写的依旧很烂），明显感觉整个人都在变懒hhh。
最后，通过和之前做微观计量对比，也感受到了每个工具的不同优势，在今后的学习中更能够通过选择合适的工具去完成任务。
Q2：列出在这个过程中最困难的，最意想不 到的地方 /本次作业做的过程中遇到了什么问题， 如何解决的？
	因为这次 Final 写的是企业层面的微观计量，数据大多都可以拿得到数据库整理过的二手数据，所有一开始觉得在数据清洗和整理方面可能不用花太多时间。但是，在后期写代码的时候，才意识到数据库拿到的数据几乎都需要重新整理，自己在数据整理方面并不是很有经验，加之这次样本量很大，所以花费的大部分时间在调整数据。
	具体解决的办法：1.在用数据前，先观察好数据的额整体情况，无论是看描述性统计还是找控制找极值，越全面越好；2.先想清楚在动手写代码（这点对我很重要……）
Q3：给未来修读这门课程的同学的建议？
	付出越多收获越多，满足感也越多。这门课很能强迫着自己去思考去学习，所以有不停的自主学习的意识或态度对这门课的学习很重要（其实考试不考讲过的内容就已经是个例证）。除此之外，有个强大点儿的电脑也很有必要 
	

	先说说本次作业我觉得做得不太好的地方吧，实在是太多了，没有完全做出自己想要的结果，现实离想象中的美好差距有点大。
首先就是我觉得我的项目相对于其他展示过的同学来说，没有特别明确的主题，比如之前展示过的那些同学，有的用来检验日历效应，有的用来做法律条文检索，而我做的更像是确定好一个大方向后，就把这个方向内所有自己想做的（并且能通过一定努力做到的）整合在一起，凑成一篇课题，所以课题名字也非常的大而笼统：“对微博大V进行数据分析”。虽说一开始就确定好了，想做和微博相关的，毕竟自己还满喜欢刷微博的，并且也找到了一篇论文，主题分析几种情感在微博中的影响力，得出愤怒这种情感最具有影响力，更容易煽动人们情绪进行转发评论等微博互动。看起来就还挺有意思的，但是我花了几天读懂这篇全英还充满不认识的专业词汇的论文后，开始研究具体模型时，发现……嗯？作者在说啥？每个词我都认识，怎么连在一起就搞不懂了？碍于我实在没搞懂这个模型具体逻辑，过于复杂，远在我的能力之上，所以我产生了自己做个独立课题的念头。
参考了前几位学长学姐的建议（不得不说，还蛮有用的），我选择了在github上找和微博相关的项目，最后确定了做分析大V微博影响力和利用主题模型做粉丝属性分析（不过感觉更多地是为了用这个模型而选取的研究问题）。本来还想做个和情感分析有关的，就是研究大V接了推广之后，用户对这一推广产品有无一定的情感转变，就连可以很方便使用的情感分析API都找好了，就是不知道代码哪里出问题了，尝试了各种方法，都没有爬下来文本数据。最后实在是太着急了，就是这几天，怕在这样纠缠下去，就没时间写最后的报告没时间复习其他科目了（我期末要考好多门……哭了），于是就不了了之了，太可惜了。
但说实话，我还算比较早开始做项目的，但就是磕磕绊绊做了好久，做到最后，别的同学在写各种期末论文，我在做python，别的同学做完所有作业开始都在复习了，我还在做python……真的是急死我了。（噢，不是说老师布置的作业挑战度太高，更觉得是自己能力不足。）不过这很大一部分原因是数据实在是太多了，跑得太慢了。所以我甚至觉得在这个作业中，我的爬数据过程比跑模型过程难度只高不低。一开始几十万条的文本数据全都拿来跑模型（甚至开始井越的一百多万条数据，一跑电脑就报错，估计是内存不够，电脑带不动），跑到晚上熄灯后，电脑没法充电撑了几小时没电了，还没跑完。于是就从几十万条一点点缩减到十万条，到五万条，最后压缩到三万条，才能在一个合理点的时间内得出结果。不过这也导致了，我最后的结果其实没有特别全面，无法覆盖到大多数，误差挺大的，甚至只能说是小部分粉丝的属性特点。
然后，说说想要改进的一些点，一是在获取博主粉丝列表时，如何跳过僵尸粉，就是当读取到该用户原创微博数为0时，跳过该用户，这样可以提高一点效率。二是停用词文档希望能改成利用机器学习自动往里面添加影响结果的词语，而非现在这样提前手动添加，因为现在的结果相对而言有些粗糙，不都精确。三是这里面中文分词不够准确，没有我看到的关于英文文档的主题输出效果好。四就是希望更加全面一点，降低一定误差，具体来说就是增加粉丝覆盖率，相对而言，这次数据虽说有好几万条，但还是不太够。最后一点就是希望能把情感分析那一部分做出来。
接着是我学到了什么。在这学期的课程中还是学到了很多有用的知识，像比如我在之前数据挖掘的课堂上有学过一些模型理论以及如何用SAS实现，不过用SAS相对就简单很多了，不用写这么多复杂的代码，基本上你选择几个选项，输入几个条件，点几个键，就可以得出结果。但在Python课堂上，又学会了一些如何用python来实现这些模型，并且相对SAS，Python能实现更多模型需求。然后通过这次作业，我进一步掌握了一些数据爬取的知识，以及一些与文本分析有关的模型，不止最后我所使用的LDA主题模型。最重要的感受是，我觉得python越来越有意思了，想在以后学到更多的相关知识，掌握更多的技能，感觉就像是通过这堂课，我被打开了新世界的大门一样。
最后是给未来同学的建议。1.在你不知道做什么项目的时候，除了参考各种论文，也可以参考github，上面有很多有意思的课题，也许可以启发你一些新思路。2.从你一开始上这门课的时候，就得开始思考你期末该做些什么了，尽管可能你最后做的和你开始想的完全不一样（hhh因为我就是，但是越早开始琢磨还是就越好）3.给您们安利这个：https://github.com/jobbole/awesome-python-cn ，python资源大全，你想要的几乎全都有。4.参考网上代码时，一般情况下都会报很多错，不过不必担心，一大半都是python2与python3版本不同的原因。自己无法解决的报错就用bing或者百度，不必太过紧张，因为到最后可能就是你心理变成运行一段代码就会在等着它会报什么错hhhhh不过debug的过程还是挺有成就感的 5.因为做这个期末作业的战线可能会被拉得很长，所以建议做一部分就写一部分自己的想法及经验总结，用以后面期末作业的心得总结，这样你会有非常多的话可以写。（噢…也可能是因为我的心路历程太坎坷了）
最后感谢60老师教了我们这么多有用的东西，人还这么好，这么有趣，尽管我最后的项目不是特别令我满意（初始期望太高），但是还是感觉这一学期学到了很多有用的知识，还吃了很多糖hhh。
（ps.写这个的时候，正好是12.31，因为不想把这个拖到明年才写完，所以熬了个夜，不过好像也算是写到了19年……我居然是做着python跨年。）
4.代码运行说明
 
如图所示，
1.weibopr.py是用来分析大V博主影响力的，同时会爬取博主微博数据保存在weibopr这个文件夹下。
2.userid.py是用来爬取博主粉丝uid的，保存在Weibo_list_p.txt和Weibo_list_jy.txt两个文档下。
3.wb_spider.py是根据粉丝uid来爬取粉丝微博的，爬取数据分别保存在weibo_p和weibo_jy两个文件夹下，这个耗时特别久，所以我又准备了一个简易的测试代码(没在图里显示出来)，名称为wb_spider_test.py ，可以爬取10条井越的粉丝微博保存在weibo_test文件夹下。
4.LDA.py是利用LDA主题模型进行文本分析的，输出为top_words.txt和top_words2.txt两个文档，前者为彭姐的，后者为井越的。
5.cloud.py是用来生成词云图的，词云形状是根据文件夹内的背景图片。
6.byKeyword.py（打叉的）是我想爬用于进行情感分析的数据结果没做出来的失败品，提交时会删除，所以不用理会。
很重要！！！说明：涉及爬取数据的代码需要本人cookie，代码中有备注提醒，不然会爬取失败。同时，在运行wb_spider.py的时候，因为爬取数据太多了，所以有可能会出现报错，只要等一会再继续运行就可以了。LDA模型那个运行时间太长了，所以我给简单地做录了屏，大致可以看一下运行情况。

本次作业做的实在是艰险、痛苦。金融方向的研究数据一般都特别大，本次论文仅仅是用到了股价的日数据就达到了400万条左右，读取时的速度以及读取中都遇到了很大困难。而在金融实务中，使用股价日数据还远远不够，我也意识到了自己电脑性能的局限性，这告诉我该换电脑了。。。
通过本次复刻论文，我意识到上课时所学远远不够。本次中令我印象最深的是groupby，该命令在面板数据中的使用实在是太重要了。而上课所讲的仅仅使用.sum等内置函数远远解决不了实际中处理面板数据遇到的问题。
第二个学到最大的地方就是合并，以right为基准或者以left为基准等进行合并，合并在金融的研究中也十分十分重要。
把python论文留到期末复习时真的是血的教训。写Python的时候还在想着自己马上就要考试了，我得赶紧复习。复习的时候又在想着python的论文要写不出来了，完了。

作为第一次接触 Python 的我， 深切感受到了 Python 世界的博大精深， 相比
于之前用过的 matlab、 Java， Python 更加开放的模式， 让我能够毫不费力地使用
他人的成果和作品比如第三方包、 API 以及各种已经完成的程序，尽管伴随着代
码风格差异和形式的多样往往让我需要从新熟悉和学习，但是这种便捷和强大让
我为发现和探索这个世界而感到欣喜。经过这一学期的接触和学习，我深知我只
不过是对 Python 有了初步的认识只能算是入门，还有很多需要学习和还没接触过
的领域，我希望以后能在这个世界各地大神云集的领域学习到更多的东西，并争
取将 Python 作为自己的一项技能不断提升。
3.2 遇到的困难
关于作业中遇到的困难，我认为最棘手的主要有三点第一、跑各个模型的时
候，参数往往需要多次尝试和不断调整才能达到比较好的效果，特别是我在进行
机器学习和统计模型的分析时，参数的调整往往对模型结果有着很显著的影响。
然而调参就需要去弄懂函数各个参数的意义，我主要是去模型程序开发者的网站
看参考文件，一般都是英文的所以也需要挺长时间。但是很有必要，因为一开始
的模型结果真的让人心凉凉。第二、在大样本下循环经常是个很耗费时间的问
题，而且有时循环之后对样本的处理出现错误又需要重新循环，就使得等待运行
的时间加倍，特别是在对样本进行清理时，往往难以提前预知会遇到什么特殊情
况使得程序中途停止，并且在大样本下找出打断运行的特殊点也是一件非常痛苦
的事。因此我在后面的编程中，尽量减少无谓的循环并且尽量使用运行速度更快
的方式进行循环，比如使用 dataframe 的 apply 和 groupby 等方法，对提高样本
效率有很大帮助，另外及时保存已经完成的数据也十分必要。第三、在股吧中绝
大部分的评论都是负面的，这就使得获取显著和高贡献度的积极情感特征变得比
较困难，这点对分类器的训练也造成了阻碍，并且导致最后分类器对积极情感的
识别准确率要显著低于消极情感，我想主要原因可能是赚钱的股民都希望闷声发
大财往往选择成为沉默的大多数，相反一般只有亏钱的股民才会来发表评论宣泄
不满的情感，因此导致每天的消极情感都要显著多于积极情感值，尽管股票升值
对积极情感值的增加有贡献，但仍不足以使其超过消极情感，因为就算股价上
涨，之前提早卖出的股民也会来宣泄消极情绪，这种现象在评论中非常普遍。这
种股民间普遍的“进亦忧，退亦忧” 的现象极大的影响了股市情感和股价上涨的
相关程度。
3.3 计划偏差
关于最后成果和计划差别最大的地方在我放弃了使用神经网络进行预测的部
分，原因主要有两点:第一、 在尝试使用 BP 神经网络和卷积神经网络时发现预测
结果很不理想，猜测原因可能在于一年内股票评论数据分布不均匀，随时间的远
近呈现递减趋势(主要是贴吧会自动更新帖子，删除旧帖子)，导致评论数据质量参差不齐，因此给高精度神经网络的训练带来困难，进而影响预测效果，其次仅
仅考虑情绪水平的模型解释变量太单一，容易产生过拟合，预测效果天然受限。
第二、由于错误低估了项目中可能遇到的问题和困难，导致时间安排不足所以没
能找到更加合适且有效的神经网络模型， 这也是此次项目中的最大遗憾。
3.4 我的建议
第一、关于课堂学习，不要抱着对待一般公选课的态度和学习方式这门
Python 课，因为它所包含的内容基本相当于一门主课，特别是之前没有接触过
Python 的同学，不要想着上课就能把 Python 学好，我个人感觉课后学习和实践
对编程的重要性远大于听课，网络是知识无边界的地方，因此信息检索和甄别的
能力对于 Python 学习十分重要。我建议以后的同学要第一高度重视这门课，第二
积极锻炼自己的信息获取和吸收能力。
第二、对于 Python 的自学部分，我认为实践是最高效的老师，而且在完成一
个个小目标的过程中能更快地学习和熟悉 Python 的使用，因此我建议同学们在
每周完成老师作业的同时可以进行一些适当的拓展，这样不仅能加深自己的印象
和理解，也能真正将课堂的内容转化为实用的技能，举一反三，方便自己以后的
学习和工作。

本次作业终于使自己用机器学习完成了数据挖掘的所有基本流程，从数据预处理到特征工程，再到运用sklearn包编写随机森林，再到最后进行基本的模型评价。
作业完成时，大量的时间花在了数据处理、完成特征工程上。翻看了大量的资料，研究如何对各个数据进行科学的编码，但总觉得主观性太强，都会损失原始数据的信息，例如：对多分类变量的处理，对各个指标到底选用那种方法标准化。纠结了很多，最后很多过程还是根据数据特点自己做了决定，不知道最后是否起到提高模型准确率的效果。在指标选取时，同时十分纠结，不知道到底选几个，最后选取前15个也是掺杂了自己的主观判断，感觉并没有可以依循的客观标准。最意想不到的是，结果呈现出来还OK，惊叹于随机森林功能的强大，准确率高，稳健性强。
因为主修金融专业，一直想尝试利用机器学习解决金融类问题。原本打算站在商业银行的角度研究银行信贷决策，但银行的客户数据基本不做披露。后面看到UCI数据库的这个问题，拥有较大量的客户信息，不用再去查找数据，于是决定站在潜在客户的角度搭建预测模型。原本打算利用随机森林，Logistic回归与SVM模型并行计算，并对比结果，但期末时间仓促，只选了调参较容易、容错性更强的随机森林模型。
给后面修读同学的意见：这门课的确需要花费相当大的精力，每周一定需要对课程内容进行复习。自己大一时学过python基础课程，所以开始还没有很吃力。但在后面讲解爬虫、统计、机器学习时，不额外购买参考书，看网课的话，第一次学基本课上听不懂，后面完成作业也是大量查阅CSDN。推荐后面学习此门课的同学，课后拓展学习bilibili上澳洲科技大学周莫凡博士讲解的Python课程（从基础课程到numpy、pandas，再到matplotlib,到爬虫，到sklearn的讲解一应俱全），同时推荐《利用Python进行机器学习实战》这本书，在实战中是很好的参考。

说实话，这门课，给我的感觉，就是让我觉得比我的三门专业课加起来还要难学。这不是因为它真正的有多么痛苦，而是在于你不知道怎么去做。我觉得这是困扰我的一个很大的问题。我只知道题目，以及我想怎么做，我要通过什么方式去做，我是一点也不知道的。在读了一两篇论文以及了解机器学习的基本思想后，我变得稍微开阔一点了。然而对于具体步骤，我还是不知道要怎么去一步一步的实现。
在这里我要实力吐槽百度以及CSDN，让我浪费了两个星期的时间。虽然我第九周就开始着手于期末大作业，但是前两周半完全在百度文本怎么预处理，文本分析的具体步骤。然而，百度只会告诉你，去停用词。CSDN的代码也只会告诉你去掉一堆停用词，然后就没了。所以前两周我就是对着我洗完后的，再用结巴分词得到的那一堆文本发呆。
后来想着这也不是办法，毕竟都不知道怎么做。便想着先去找一下别人做过的文本分析案例来看看，百度半天，没有，有也是那种很简单的情感分析。随后，陷入绝望。
一直以来非常感动老师的鼓励和支持，其实一开始我不敢选这个题目，老师说“很简单，而且很有趣。做吧！”在老师的鼓励下，我就着手做了。
老师给我回的邮件对我的进展带来了质的飞跃，首先是我找到了一个更好的数据，不再是我原来自己的那个数据了，它已经经过一遍清洗了。因此，找到这个数据是我的第一大信心来源，毕竟我就不用去使用让人倍感恐怖的正则表达式了。
我们班大佬张见阳同学也在这一路给了我很多支持，在我安装不了某些包的时候给我安装包，给我找错。重要的是给我指明了方向。在张同学的推荐下，我选择了先把keras看了一个大概。在老师的指引下，我去github上面找到了较多参赛者的分享。得到了他们的大概思路。而我对于json格式一无所知，对CNN一无所知，一开始我想自己写，但是挣扎了一周后还是没办法开始去理解一个参赛者粗糙、不全而又难懂的代码。
虽然在参考别人的代码的时候会万分痛苦，我看了以后，自己理解用自己的想法写出来。对于参赛者很多函数的参数设置的我很不理解。但是还是要坚强的看下去。参赛者用了很多我见都没见过的函数和方法，我都会在bing上面一点一点的查找，运用。虽然说我的代码是函数部分参照是参照参赛者写的，但是由于我的很多函数和他不一样，而且他直接把函数定义在了类里面（我最近才发觉这个问题），因此到后面的时候我更笨无法参照他的代码，只能是理解他想怎么做，然后自己写。
最为痛苦的莫过于自己检验自己写的函数，不停的报错，就一直不停的找错。而且有一些函数是函数套用函数，因此就会在套用的时候出现错误，可能是类型的不同，也有可能是其他。不过当看到自己把文档一点一点的处理，结果一点一点的出来的时候，真的就会特别开心。
其实一开始并不知道为什么作者要这样做，只是我不知道怎么一步一步去实现，所以也是在一点一点的跟着作者做，一边做一边读入数据来验证函数。到了后面自己对文本进行剪切，分段序列化的时候，就发觉这种方法的强大了。作者还定义了 一个类，我也跟着他定义了一个类，感觉十分方便。以至于我在后面的过程中，可以有一些自认为更好的想法，并把它写下来。
在上面的这些过程中，作者用到了很多我没见过或者不是很懂的包和函数，当我看到没见过的包或者函数时，会搜一下其使用方法，再自己举例来用一遍。比如map()、pickle.load()、np.zeros()、np.inld()、keras里面的各种包的引用包等。
当然，如果要问我收获最多的是什么，我觉得应该是一种学习方式的转变吧。这应该是我学习方式的第三次转变了。第一次在小学，我知道不会的题目要去向老师求问；第二次是在高中，我知道不会的题目可以和同学讨论；第三次就是在这个学期，我知道不会的问题可以自己在网上寻找答案。
可能是专业的原因吧，我从上大学，一般不会的题目就会选择向老师和同学请教。而在这学期学了这门python后我发觉好像不太行，有时候助教还没有回复，我自己就已经在网上找到了答案。而且计算机似乎和数学不太一样，它需要知道你的整个逻辑，它所研究的问题不限于统计学的那几种方法，每个人都可以写出自己的方法，以至于别人根本没有办法检查出你的错误。
通过这一段时间的学习，我觉得我的解决问题能力、信息搜索能力、耐心、防止焦虑的能力都得到了很大的提升。
由于我十六周有两门专业课的考试，我就很想在第十四周把代码部分结束。可是当我发觉十三周周末了我还卡在循环遍历打标签的时候，内心真的是十分着急。而当我发觉我写了一个早上的函数，还是在不停的找错的时候，内心真的十分痛苦，即将崩溃。在老师十分耐心的引导下，我平静了许多，但是当老师说出鼓励的话的时候，我的泪水就莫名其妙的淌了下来。
第九周到十五周，真的是一个内心焦虑无比的过程，我还记得我十一周的周一担心写不完代码而失眠到凌晨三点半。那时候的我特别想自己写出来，但是又不知道从何下手，只有干着急，每天的进度也是很慢。那时候，真的是一种折磨，我专业课上都是一边听课一边装包。
是的，我的困难不仅限于写代码，还有装各种包。在经历了安装多个包以后，我安装包的能力也得到了大幅度的提升(再一次感谢大佬张见阳同学)。装包道路坎坷，我就挑选几个比较折磨人的包来说一下吧。
第一个是jieba ，我也不知道为什么我pip安装不行，conda里面安装也不行，询问助教说可能我安装了两次python。后来没有办法，我是按照网上的做法，一点点输入命令，才得以完成。后来电脑也不知道是什么情况，就用不了了，无奈之下我只好选择去重装了一次系统。装完系统后，结巴倒是安装的很快，可是tensorflow怎么都安装不上。我忙活了近三个小时，复制错误也没用，再一次感谢张见阳同学，给我弄了快半个小时，终于找出了原因，原来是因为我的conda版本太新了，tensorflow不支持这么新的版本。
代码一路写来，可谓是一波三折，都不知道自己写了多少行。这一页太乱了，就把写好的copy到另外一页上面继续写。在换了N多页面，实践了多个函数以后，终于弄完了。然而，某些调用函数还是会报错，后面又慢慢调。从第十周开始，特别是周末，我每天一起床就开始去自习室写代码，中午吃饭只需要20分钟（恢复到了高中水平），每天学习时间可以长达十一小时。
这门课，学的十分心累。但是，等到走完这条路，会发觉，好像那些自己以为很难的东西，似乎也不像自己想象中的那么难。而你自己，似乎也不再是那个你。面对问题，你不会再干着急；面对困难，你不再是想着依靠别人；面对代码报错，懂得了怎么去解决。
现在其实这个作业对于我而言还没有真正的结束，首先便是CNN模型的原理是什么，怎么去写出一个CNN模型来，这是我后面想继续探讨的。其次是我用CNN做出来的效果其实不太好，我在后面有去找过其他参赛者的RNN模型来套，但是由于函数等的不一样，完全套不上去。临近期末，大佬也很忙，我的时间也很紧，我就没有去尝试这一种做法。所以说我想探讨的另外一个问题就是如果用RNN，效果会不会更好一点。
最后一个很想去继续探讨的问题就是，如果我拿到的不是比赛那样那么好的数据，而是我自己找到的那个数据，我可以通过什么方法去解决这个问题。在做完这一次作业后，我能不能有自己独立的想法把这一个问题完完全全的做出来。
真心推荐，虽然这门课根本不是一门专业课（每周有作业，还有测试）。但是，在你上完这门课以后，你会发觉，你，已经不再是当初的你！
最后，再一次衷心的感谢班级大佬张同学的帮助和老师一路以来的鼓励，不胜感激。

. 学习收获：
（1）增强阅读文献的能力
不管是在前期的选题，还是中后期的实践中，对外文文献的阅读对项目进展很有帮助，而我也在过程中明白，阅读文献更重要的是读懂其逻辑，理解脉络。
（2）提高搜集数据的能力
	从初次确定选题，到换题，再到立题，这个过程中最重要的影响因素就是数据获取。初步想法本是找到原论文的数据，但是由于原文是通过抓取伦敦单车在线网站获得，无法与其相同，故选择国内数据，最终在2017年摩拜杯比赛中找到可用的类似共享单车数据。
（3）编程能力扩展与巩固
	在上这门课之前，我对python只是处在大致了解阶段，平时多用matlab和java，但是入门之后发现其强大的扩展包深深的吸引住我。通过大作业，我对python+panda+matplotlib的操作结构逐渐掌握，从只会老师上课讲授的部分技巧到融会贯通，而大作业中也用到了大二所学的javascript，我对于语言间的相结合能力也得到了提升。
（4）掌握数据可视化的方法
	本次大作业中用到了多个数据可视化的包，比如matplotlib、seaborn，有助于对于数据状况的呈现。而在对曲线进行平滑去噪时，scipy中的smooth方法也有效地帮助数据进行可视化。对于热力图的显示，最开始的想法是利用一些包绘制静态热力图，但最后采用调用百度地图来进行动态热力图的展示。
（5）掌握百度API的调用
	利用百度API绘制动态的热力图，并学会在python里运行javascript语句。
（6）地理经纬度编码解码的方法
	初始数据的地理信息给的是geohash编码，从一开始的不理解，到查阅文献学习地理经纬度编码，再到准备自己编写解码函数，过程中认为应该开源包进行解码，最后终于找到并成功运用。
（7）大数据处理中聚类算法的应用
	DBSCAN层次凝聚法运行效率很慢，且无法明确知道聚类中心，但是符合基于密度的地理经纬度聚类。而KMEANS运行效率快，但明确知道聚类中心。而这两种都只适合数据量小的情况，对于本次大作业中多达321万条数据处理较为吃力，最终采用了对地理经纬度进行预处理划分，再进行聚类的方法。而大作业中采用的聚类是DBSCAN与KMEANS的结合算法。
2.最困难的，最意想不到的地方
（1）如何进行地理经纬度的解码
（2）如何实现原文中NAB(单车站点空闲度)的类似替换
（3）如何绘制动态热力图
（4）如何实现大数据聚类
3.所遇难题及解决办法
（1）原文NAB研究的类似转换：
采用CNT_use替换NAB，研究空闲的反面——利用次数，来表示工作日与休息日共享单车利用率的变化情况。
（2）如何绘制动态热力图：
	查阅到利用百度地图API可以将地理信息的json文件绘制成热力图，但是因为调用百度地图没有pythonAPI，而老师要求只能使用python进行编程，故想到采用webbrower包加载写着javascript语句的string，以此实现API的调用。
（3）数据量太大无法聚类
	最开始对全体数据进行聚类，出现多次内存溢出情况，查阅文献后发现必须对大数据进行预处理，而每一次聚类的最大数量大致为13000，故对每次聚类的数据进行分组聚类，再把聚类中心给合并后写入json文件。
4. 相比计划的最大差异
最大的差异就是研究成果的可视化部分：
（1）	本想采用NAB，原文中利用NAB值从站点空闲率的角度进行对共享单车利用率的研究，而我的研究中从AVG_distance（骑行平均距离）、CNT_use（使用量）的正面角度来对共享单车利用率进行研究。故而所绘制的曲线也不同。
（2）	本来想绘制静态地理信息，但是最后画了动态的热力图。
（3）	关于猜想工作日、休息日共享单车使用情况的结论不同。
5. 给未来修读这门课程的同学的建议
（1）一定要对大作业早一点打算，不要等到还有一个星期才开始做
（2）要对自己的能力量力而行
（3）如果选择对地理信息进行聚类的话，可以选择使用基于四叉树的快速层次聚类法，在获得聚类中心的时候可以采用KMEANS+DBSCAN结合的方法。
（4）遇到困难不要放弃，多去论坛上或者搜索引擎上问问，也许就会灵光一闪，解决办法就出来了。
（5）上课要多听老师的点播，也许你会觉得课上的知识很少，但是作用就恰恰是抛砖引玉，师傅领进门，修行靠个人，对于数据科学的领域，还有很多的秘密等着我们去发掘。

（1）自学能力的提升
在我做这个项目的过程中，遇到了无数没有学过的东西，所以自学成为整个
实验过程中的主要工作。
在读取数据的时候我就遇到了全新的数据类型——json 数据。之前在这学
期学习的过程中，主要都是读取 txt 或者 csv 文件，然后用 pandas 来进行数据
的处理分析等工作，对于 json 数据从来没有接触过，所以当我刚拿到 json 数据
的时候，甚至不知道该怎么读取以及调用。成功读取文件之后，由于对 dataframe
类型的数据比较熟悉，而且我也觉得这种类型的数据比较直观，容易看懂，所以
我就将数据转换成了 dataframe 类型，但是在后续处理数据和跑模型的过程中出
现了很多问题，最终我还是直接调用了 json 数据。其实后面随着我对 json 结构
的逐渐了解和熟悉，我才发现这种结构本身就很方便对它进行其他的操作，一开
始我觉得它很难，完全是因为我对它的不了解。
读取数据之后的数据探查部分比较顺利，基本都是运用课程内所学到的知识。14
但是等到数据清理的过程中，中文分词这样一个新的“知识点”出现了。相比于
英文分词可以自己写代码用标点符号和空格进行切割，中文分词更多的是依赖于
各种别人写好的包。在查找分词工具的过程中，我发现了很多感觉比较靠谱的分
词工具，比如 jieba, THULAC， foolnltk 等，并且我也安装好了很多包，准备用
它们全部跑一遍，自己实验一下到底那种分词工具分词效果比较好，分词的速度
比较快，但是由于数据量较大，分词所花费的时间远远比我想象得多，因而我只
用了 2 种分词工具加以比较。
在跑模型的过程中更是遇见了无数的新知识， 先是特征提取过程中就有
word2vec,Tfidf 等方法，后续的分类模型也有朴素贝叶斯分类器，逻辑回归模
型，随机森林模型， fasttext 分类器等等， 这其中的每一种方法都需要自己先去
了解它的原理以及怎么应用，然后直接进行选择或者对其中几种进行比较分析。
所以在整个实验过程中，自学花费了很多的时间与精力，这也让我的自学能
力有了较大的提升。如果说以前面对新的算法和模型，可能心存一丝畏惧，感觉
很难，担心自己学不懂的话，那么经过这次的实验， 我觉得自己至少不会把自学
当做一件很难的事情了，无论我以后面对的算法和模型有多复杂，我相信自己至
少会敢于并且愿意去尝试接触它们，毕竟如果自己真的理解了那些算法和模型，
感觉还是一件很有成就感的事情。
（2） 对于文本分类有了更深的了解和兴趣
自然语言处理一直都是我很感兴趣的东西，因为觉得让计算机来理解人类的
语言是一件很酷的事情。但是对于自然语言处理领域的诸多应用，如语义分析，
文本分类，文本生成等，其实很少自己实践。 虽然以前在我们学院实训过程中曾
经做过情感分析方面的工作，但都只是用到了最简单的模型，而且其实最后的情
感分析结果准确度也不太好。而这一次的实验，让我对文本分类这一部分的研究
有了比较深的了解。在实践层面，我应用了一些模型，也让我直观地感受到了不
同模型之间的各种差异；在理论层面，虽然最后我自己用的模型只有一些比较传
统的机器学习模型，没能用到深度学习的模型，但是在前期看文献的过程中，我
其实看到了挺多比较前沿，比较复杂高级的算法和模型，也希望以后有机会能够
应用它们。
7.2 实验过程中的那些坎坷
（1）电脑黑屏
由于我这次实验的数据量还是比较大的，而我的电脑之前由于安了很多软件，
所以容量有点不足，在一开始对数据继续分词的过程中电脑黑屏了很多次，这是
我之前完全没有预料到的。好在后面在我删除了一些很大的软件之后，后续的实
验过程中电脑就比较正常了。
（2）特征向量维度较大，跑模型时间很长
在我一开始进行特征提取的时候，为避免漏掉一些具有独特性的特征，我只15
对 df 值的上限进行了限制（如果某个词在超过 50%的法律文本中都出现过，这
意味着它不具有区分度，所有把它筛去）。然后在跑随机森林模型的时候，我就
发现一开始我用小样本进行代码测试的时候它可以正常出结果，但是当我开始用
全部数据跑模型的时候，一个多小时它都跑不出结果。所以后来我就对 df 值的
下限也进行了限制，从而降低了特征向量的维度，模型才跑出了结果。
7.3 致未来的学弟学妹
（1）留够时间很重要
我一开始在规划我这一次的项目的时候，其实想做很多事情，比如分词工具
我多选几种进行比较，特征提取过程我也把常用的 3 种方法都测试一遍，分类器
也要选择 4 到 5 种加以比较。但是后来真正开始做项目的时候我才发现，无论是
分词，特征提取还是跑模型，都需要花费很多的时间。时间方面的限制让我不得
不对内容进行了删减，感觉也是挺遗憾的。
（2）文献和论坛相结合
在实验的过程中，一开始了解这个领域的研究现状，研究方法，理论层面的
知识等，肯定是通过文献来了解比较好，虽然文献看起来略为枯燥，但真的质量
很高，全是干货。而且看文献的时候要尝试看英文文献，我一开始在知网上看中
文文献，找到司法智能方面的文献很少，后来在微软学术搜英文文献才发现有很
多。
在真正开始写代码的过程中，其实文献的帮助就比较小了，对于具体的模型
应该怎么调用，算法应该如何实现，爆了错应该怎么实现，论坛上的内容能够在
代码方面起到更好的帮助作用。
（3）理想与现实的差距
我是没有复刻论文的，而且我想做的事情都有点天马行空的感觉，一开始规
划的太多，考虑的时候不够细致，让我在实践过程中遇到了很多的问题，所以对
于这种理想与现实的差距要有一定的心理准备。但是在实践过程中，其实也会不
断联想到一些新的东西可以做，这样积极的“理想与现实的差距”也会让人有点
小开心。

本次作业新学到的是两个包的使用：jieba和gensim。使用Word2vec进行文本的词向量分析可以说是非常好用了。当然“改错”能力也有很大的提升。
跟之前计划不同的便是高估了自己的能力，直到自己越想越难，和老师交流后真正意识到问题并及时调整，选择了力所能及的作业难度。
最困难的便是或大或小的不知名的错误。现在看来，中文显示不出来真的算是芝麻小事了。结巴切词时而打印出来时而打印不出来；明明代码一样第一遍运行写入的人物名始终不完整，非要第二次运行才完整写入文件；代码没错显示出来的颜色确实从来没有见过的，甚至不如自己搭配的色组等等。对于大多数问题其实在网上多查一会都能找到相应的解决方案。但对于若隐若现的自定义词典已经没辙了；代码分明没有变动结果却不同，从不断的怀疑人生到怀疑电脑，最后确定肯定一定是电脑的问题。不得不说这个颜色不同，机缘巧合之下看到了一款seaborn特别好看的渐变色组，于是努力的想换上新的“皮肤”，最开始出来很丑的颜色我一直怀疑自己的代码不对，于是各种查代码。结果就是花了两个小时的时间加各种参数各种改代码加注释，颜色却坚强的一点没变。最后放弃挣扎了，自我催眠式的认为是各种版本不对的原因。
最最绝望的却是从.ipynb到.py的运行。本来以为都是一样的代码只用改个后缀就可以了，以为1个小时绰绰有余。事实证明自己真的太年轻。在“相关性分析.py”上花了一个下午，始终报错“word not in vocabulary”，不停的查资料在各个地方输出，在5点多的时候运行了5遍+之后终于不知道为什么又显示出来了，于是激动（作死）的关掉Spyder重新运行一遍，眼看着最开始的ddl已经过了还是显示不出来，只得屈服，在jypter上运行截图。（我至今不知道为什么jypter上运行很成功转在Spyder上却会报错）。
（代码多次运行后有内容会有变化我以最后一次的截图为准了）
给未来可能会选课的你：
有趣的老师+有意思的上课内容=非常值得选的课程
这门课大概是大学上课以来最不“水”的公选课吧。身边有人惊叹公选课怎么这么累，但依旧坚持选择修读因为真的很有趣很有用。
不得不说老师每节课的“糖果”真的是很大的激励了，虽然并不是每次都能吃到糖但我认为只要哪怕一次，当你吃过这份奖励，你会愿意把这份兴趣保留到课程结课的那一天。
其实我的编程并不是很好，印象最深的是一次测试，当天晚上12点的ddl，12点过了却还在一个错误上出不来。我知道那个错误其实应该很简单，但当时的自己真的压抑不知道到底怎么改（还记着老师说不要问同学差点崩了）。但幸运的是凌晨2点终于改对了，提交完的一瞬间便绷不住了，哭了又 笑了。至少自己独立做到了，虽然花费的时间是别人的几倍。
很喜欢老师的一句话“代码你们都可以上网查，不闭卷。”有人吐槽老师难度太大，其实老师在开学就强调很多次这个课的难度不低，没有编程基础慎选（有基础也要深思熟虑，来自过来人的忠告）。如果你的编程能力没有那么优秀，建议找到同学一起上课，能够交流。因为有的时候编程上的问题自己一个人很容易钻进牛角尖。但其实你也不用担心，老师和助教真的真的超用心的，代码不会的地方都超耐心的解答。
大作业真的真的要提前做，否则期末别人都在复习你还在写代码怎么办。还有就是没有提交之前一切都有可能发生，所以真的要尽早！！！
不管怎么样，选课的你们一定不会失望的。

大作业写到这个部分， 可以说是让我松了一大口气。 经历了前前后后两个月
的准备， 三次题目的更换， 模型的增减， 到现在的定稿， 这真的是一件极其耗费
时间和精力的事情。 但是， 可以说有付出也有收获吧， 现在弄完整个大作业的我
甚至觉得自己这篇 paper 可以再利用接下来的时间进行修改， 然后发表~哈哈哈
接下来， 我将从课堂学习和大作业的完成两个方面进行个人总结。
一、 课堂学习
这门课是一门很有难度的课程， 在这学期一开始就从老师不断劝说退课的过
程中深深的体会到。 但是作为一个一心向学， 想要学点技术的我来说， 还是义无
反顾的留了下来（虽然在写大作业， 模型一直不成功的时候死亡式后悔）。 整个
学期在老师的带领下， 开始了有条有理的 python 做数据分析的生活。 从一开始
基础的 python 语言学习到后期的数据获取、 数据清洗和图示化建模， 在老师快
速的语速和丰满的内容下， 一学期被安排的满满当当。 作为被程序员誉为“最美
丽的” 编程语言， 语法结构简单， 逻辑清晰。 有大量可调用的包以供大家使用，
但是这种魅力在本学期的课堂学习中其实感触并不深刻。 因为普通的语法学习已
经占用了大部分的时间。 所以作为一个有编程基础的人来说， 还是希望能够多学
习一些和数据分析方面相关的内容。 因为在后期大作业完成的实际过程中， 基本
属于码代码的共性， 码一个查一个 de 一个的状态。 还有可能就是个人英语能力
有限制的原因， 在整个学习过程， 阅读英语文献耗费了大量的时间， 导致效率很
低。 所以还是希望老师在推荐专业前沿的英语文献的同时， 能够推荐一下基础性
的中文文献。 因为感觉对于我们这种基础性学习的” 求学者” 来说会学习的更轻
松一些。 总的在课堂学习的过程中还是收获了很多， 可以算是一脚踏进了半个
python 门。
二、 论文写作
论文写作的个人总结， 为了有个清晰的思路， 预计按照选题、 数据获取、 数
据清洗、 建模与描述性统计、 图示化五个方面进行总结。
1. 选题
在开始本门课的大作业之前， 我一直跟着老师坚持每周阅读文献。 所以在一2
二次选题的时候选的原 paper 都是全英文， 处于能看懂的状态。 但是看不明白看
不深刻。 再加上第一次 paper 的数据获取方式不符合要求、 第二次 paper 的数据
获取全是靠下载（担心工作量不足）， 所以都被放弃。 终于在还有四个星期的时
候更换到了最终做的这个 paper。 由此看来， 首先要选择出自己喜欢、 感兴趣的
paper 是一件不容易的事情， 还要有工作量、 作业要求的限制更是难上加难。
2. 数据获取
本次数据获取最大的难点是百度指数的爬取。 在最开始选题时， 就听说这是
一个大工程， 有意思， 工作量足够。 于是在还有四个星期的时候紧赶慢赶开始了
百度指数的爬取过程。 百度指数实现了对搜素数据的良好统计， 可以根据需求查
出一切你想要查的关键词， 还可以区分地区和搜素方式。 数据是很全面了， 呈现
方式也很动态美观。 可是全部数据储存在图片中。 这就涉及到了图片的识别。 百
度指数还加入防刷的控制， 不断要求输入验证码， 拒绝访问。 在心态崩溃的边缘
来来回回弄了一个星期。 抱着笔记本跑向计科各大佬所在地进行求教。 终于在大
佬这句话的回复下我选择了放弃自己爬取。
然后我开始了各种搜集百度指数的过程， 上淘宝查价格， 按照我的需求量大
概要用 100 元（哭了）， 又开始尝试各种 api 和下载软件。 终于功夫不负有心人，
我拿到我想要的数据。
比起百度指数的获取艰难过程， aqi 指数和天气状况就显得很简单了（虽然
也不简单）， 中规中矩 debug， 中规中矩看页面， 中规中矩得到数据。
3. 数据清洗
拿到了全部数据之后， 自然开始了数据清晰的过程。 整个过程基本感觉自己
脑子废掉了。 全部用基础性的 for 循环， if 语句， apply 方法完成了整个过程。
数据结构中学到的各种算法全部没有派上用场， 导致整个文件的运行速度极为可
观（难看）。3
4. 建模与描述性统计
建模是整个研究的重点， 也是我花费时间最长的地方。 在这门课学习之前，
有过做数据挖掘的经历， 但是都是使用的 spss， sas 等软件。 简单的图形化操作
让整个分析过程看起来也不是那么难。 但是现在需要用到 python 了， 里面的各
种包都让我感觉云里雾里。 然后本研究中又涉及到了控制变量和哑变量， 我更是
不知道怎么加。 在艰难的做出原本 paper 的分析过后， 来到了断点分析阶段。 在
网上查了好多天的文件都找不到 python 做断点分析的方法， 没办法。 报上电脑
找大佬、 找老师。 老师说目前断点分析还是主要使用软件进行， python 做难度
有点大。 于是在老师的建议下， 立马决定换方向， 对已有的模型进行“ 升级”，
实现预测分类模型的建立。
5. 图示化
图示化的过程还比较轻松， 拟定好想要画出的图像， 调调代码就出来了。 整
个图示化过程无亮点， 也是我整个过程的一个遗憾。 毕竟作为一个设计狂魔， 还
是希望有一个漂漂亮亮的图形的。
总的来说， 这学期这门课课堂学习让我 python 有了基础认识， 后期大作业
让我更加深入的了解它。 Python 的强大是无疑的， 我相信它之后的运用场景也
会不断扩大。 对于这门课来说， 我最深的感悟就是， 非常有用。 但是一定要选一
个课程压力不重的学期来好好深入学习， 才能实现学有所成。 最后， 感谢老师的
教导

这次大作业，做了好久好久，整个过程简直就是一部心酸血泪史。不过还是颇有收获的，我这篇论文主要用到的是爬虫、回归以及用pandas清理数据。在爬虫这部分，又学到了如何使用selenium虚拟浏览器来进行爬虫，这个部分和上课时学的爬虫方法相比，能爬回来原方法弄不回来的数据，而且能直接对页面进行操作控制。回归方面，学到了一个新的模型向量自回归，也练习了用Python做回归。这是第一次完整的复刻一份论文，从选论文、读论文、理解论文、进行相关调整，找新的研究问题，这是一个整体的过程，完整做完一个项目不只是和Python以及数据分析相关，还有很多别的内容。这绝对是我读大学以来最艰难的挑战了，从对Python的陌生（还会犯语法错误的那种），到后面狂查资料，去学习更多关于爬虫与数据清理的方法与知识，能独立完成一篇论文的复刻，真的有一种升天的感觉，也体会到了程序员的艰苦！不过还是觉得非常有收获的，课堂上老师能教给我们的具体方法有限，更多的是引我们入门，尤其是编程这方面，不断有新的方法出来，很多问题都有别人的经验可以参考学习，学了这个课自学能力得到了极大的提升。
在完成作业的过程中，经历好多好多困难，我一一来说。
首先是选题，老师说要选一篇高水平论文，我当时也不太清楚怎么定义什么论文是高水平论文，就去找了一个大学者的论文，当时读了那个论文之后，看到文章的数据来源写的很清楚，我就开始写那篇文章的总结，那篇文章的理论还有点复杂，我光是理解论文就花了很长时间，等准备开始着手找数据时，才发现那个数据实在是太贵了，实在是拿不到，所以只能换题。
这次换题的时候，就很注重数据的可得性，就选择了股市的数据。就以股市为关键词去找A刊的论文，然后就选到了现在这一篇，股市的数据好获取，财经新闻的数据原文就是用的爬虫，正好与Python关系密切，所以最终选择了现在这篇论文，但这只是痛苦的开端，由于换题，耽误了很长时间，重新理解新论文又花了很长时间，导致后面真正做论文的时间被大大压缩，也使得我可选择的数据范围大大受限。
之后我就开始找数据了，当时选数据就面临困境，如果是选择快讯，数量是够多，但是没有其他维度的信息，但如果选择长篇新闻，数量上又会受到局限。比较了好多个网站，只有同花顺财经提供的新闻能大致符合我选论文的要求。但是由于我先前换题，时间没有安排好，所以等我开始去抓取同花顺的数据的时候，网站能够提供的最早的数据的数据也知道11月29号了，把我的数据范围限制的死死的。
确定了能拿到的数据范围后，就开始爬数据，刚开始使用上课时学到的爬虫方法去爬的时候，虽然是显示抓回来了，但是实际上什么东西都没抓回来，后来才知道是因为没有header被网站发现了。后面抓取二级连接数据的时候，又遇到了新的问题，点赞、评论等这些动态数据虽然在网页“检查”时显示有数据，但在抓回来的源代码里面又全都是0，后来老师说可能是用json文件存储的，直接抓抓不回来，我就去查怎么处理json文件，但是我始终找不到存放我需要数据的json文件，所以只能换别的方法。后来助教学长让我尝试使用selenium抓取，这一次终于成功抓回来了数据，等我满意欢喜要去把所有的数据都抓回来时，试了一个数据集，结果什么都没抓回来，全是nan。但是找代码的错又找不出来，后来就一条、五条的试，一条一条试有时候可以有时候不行，五条的时候到还是正常，后来学长说可能是浏览器不稳定，所以我本来是要一次性抓完一个数据集的，现在只能分很多次来抓，一次抓200条，抓了一定数量之后还抓不回来了，应该是网站有限制，只能再停一段时间之后再抓。总之，经历了很多波折，好不容易才把数据都抓回来，还有一些奇奇怪怪的现象到现在我都没搞清楚原因，比如说有些代码写成函数就抓不回来想要的信息，但是把函数的格式去掉后就能成功，所以我也只能不用函数，直接爬虫抓数据。
和之前的计划相比，最大的差距就是时间安排了。本来是一周一周安排的，但实际上很难按照计划来安排。还有一些别的课程也还有很多内容，忙起来进程有时候就耽搁了，加上对于这种工程浩大的作业很多问题都是做的过程中才会碰到，之后才能想办法去解决，所以这一次的作业我实际上并没有控制进度的能力，只能是一直在这上面投入时间，过程中也很焦虑，只能尽快去处理该做的问题，一步又一步，有点像是和时间赛跑。
最后想聊一聊的就是给以后选这个课的同学一点建议。首先就是时间，一定不要低估大作业的难度，不然最后你可能真的会崩溃，所以一定一定一定要留出足够的时间来准备！第二个就是选题要谨慎，我这次选的题就是越做越觉得原文的研究有问题，但是那个时候换题已经来不及了，所以不仅要开始的早，选题也是要好好研究，保证没有比较严重的问题。第三个就是要习惯写函数，写函数一开始可能觉得麻烦，但是用函数真的会有较大的帮助。写注释同理，写完代码的短期可能还能记得住，但是过个几天可能真的就分不清楚了，所以写注释真的会很有用的。当然，选这个课，胆子要大，也要有心理准备与过硬的心理素质，不过老师说得对，做完这个作业真的会有很多收获的。后面选这个课的朋友们加油吧！选了这个课完成了大大小小的作业，收获绝对和旁听上课不做作业有很大的差别。看到自己的代码不仅不报错，还给到了自己想要的结果，那个时刻真的会很有成就感的！

深度学习理论文章比较晦涩难懂，去观看MOOC上吴恩达老师深度学习的视频去补习神经网络的相关概念。找到一个blog，做的内容是基于 Word2Vec 词嵌入&LSTM英文文本生成，文本原材料是丘吉尔自传，然后我发现了神奇的keras 模块！是搭建在Tensorflow模块之上的封装更好的模块。看了《TensorFlow Keras 深度学习人工智能实践应用》里面都是具体的实例配合代码的讲解，繁杂的数学原理只是通俗易懂的简单介绍，大概了解了keras的使用方法，“搭积木”式的网络构造方法也是简单易懂。
刚开始代码慢得令人窒息，训练一个epoch用时就是4000多s，慢的离奇，那个博客中的人训练了50个epoch为了保证效果，我一开始也设置了 50，但是照这个速度下去至少要跑好几天。文本字数是11217046，然后每个字有自己的 100 维向量，我的想法是训练集是 11217045 个字向量，标签集是 11217045 个字向量（我认为的根据前一个字预测后一个字）。这样的话浪费存储空间比较多，生成这两个集合的过程就需要大约 5 分钟左右的时间,而且在构造训练集和标签的过程中，系统就提示我，电脑内存已经被占满，请立刻关闭当前进程，显然 4G 运行内持不了这么大的数据量。好在我本身的电脑的运行内存是 8G，勉强能让程序进入训练状态，所以我就放弃了云端跑程序的尝试，又回到自己电脑上运行。后面的代码跑了8个小时的样子，已经很满意了。
数据可视化有点丑emmm，安慰自己是莫兰迪色吧。
换选题：在经历过连续20多个小时的打击折磨之后，我很没骨气得放弃了爬知乎！然后想到一个月之后放假，沉浸在回家的喜悦中，兴冲冲去爬特价机票，然而爬了一些数据如果只是做机票检索没什么意思，不如做动态规划航空定价预测机票价格就又放弃了。在父亲的启发下做了诗歌的生成。
如果以后选课的同学能看到：
1.	如果学期专业课本身比较忙慎选！在这门课上的时间要比四门专业课加起来都多（别问我怎么知道的……）
2.	多问老师！多问老师！多问老师！老师人超级好的！不要不好意思！（老师有多好呢？我就不说了，免得大家像我一样给老师添麻烦emmmm，鞠躬！道歉！感谢老师！）
3.	自学能力很重要啊！刘凌老师上课比较关注学生的长期发展，介绍的知识范围会很广，可能这与语言本身更新快也有关，就需要大家自己在变成的过程中多体会语言细节，尤其是类型，很多时候找不到错误问题就出在这里。

本次作业学到了什么
这次作业几乎把我所学或者有所了解的python相关的知识都用了一遍——爬虫，pandas数据清理，分词向量化，神经网络（还差统计回归那块）。但说实话，这次作业更大的意义，在我看来，不是我学到了什么，而是让我知道了我还需要学什么！就分词和向量化而言，我现在仅仅局限于调用Gensim上的函数，对于其中的具体算法全然不了解。就神经网络而言，虽然经过这次作业，我从数学上对神经网络有了更深入的了解，也知道了CNN，RNN等等深度神经网络应用上的区别及其原因，但是认识仍然停留在表面，细节性的东西还没理清楚，现在似乎离亲自实现一个BP神经网络都还有一段距离。并且在金融中更流行的随机森林和深度森林等网络以及其他机器学习方法诸如SVM都还不是很了解，可见在机器学习这一块，这次作业算是给我打了个铺垫，至少让我和别人聊起机器学习来不至于冷场，但前路漫漫仍需努力。此外，这次作业也更加让我意识到学习Linux的迫切需求，我的这个作业都是在一台Linux服务器上做完的，在最后完成阶段，为了把根目录里面所有文件夹打包下载，我还费了点时间去网上找Linux相关的命令。 

这里推荐一个深度学习入门的博客，讲得非常好：
https://www.zybuluo.com/hanbingtao/note/476663 

困难的地方
1.	理清思路：
说实话，理清思路真的是我认为最最困难的地方了。本来我想着做这么一个情绪指标应该挺容易的，但是仔细想来，要把其中的步骤理清楚确实是一件非常麻烦的事情，首先是数据清理这一块，如何防止重复性工作，顺序特别特别重要。其次是到底选择用什么方法进行情绪指标的构建：虽然在课堂上我已经建立起了一定的概念，但仍然比较模糊。于是我在网上找相关步骤，我发现网上的说法真的五花八门——总结起来大概是一种是基于情感词典，一种是基于深度学习（但网上很多地方都混在一起讲不清楚）。后来我去淘宝上买了一本书《python自然语言梳理实战——核心技术与算法》里面倒是罗列地很清楚，从分词，向量化，到深度学习都所有阐述。

2.	搭好深度学习的环境（GPU版）：
这个真的麻烦，因为连带要安装的软件太多以及版本问题，导致一开始根本抓不清楚安装步骤。

差异与遗憾
1.	因子较少
本来在情绪因子的选择上我还打算加上换手率的，然而我发现我愣是没办法从tushare、万德、同花顺上把这100只股票的每日换手率一口气下下来，然后我放弃了。
2.深度学习结果的准确率不够高
说起这个问题，我倒觉得很有意思。现在我上交的作业采用的是4分类，这个4分类的准确率只有可怜的40%（虽然比瞎猜要好），但其实我之前最早是采用5分类的，5分类的准确率竟然也有40%。虽然准确率比较低，但是从最后的结果上来看，4分类和5分类得到的情绪指数的图像却没有什么差别。（我从5分类改成4分类的原因是，我觉得四分类比较容易统计二分类时的准确率）
3.标签构建
这里我的标签样本的标签是通过表情符号打上去的，我选择了使用频率前40高的表情符号，出于人力有限，我只为并为每个表情符号选了50条评论，人工地对总计2000条评论打情感标签，但实际上每个表情符号50条评论是不够的，一般至少得有200条以上才比较准确。

给未来同学的建议
这是一门真正喜欢编程的同学才适合选的课。个人认为，编程这门技术从来都不是在课堂上就能学会的，她需要课后大量的付出，只有自己平日里多思考，多写代码才可以。她在平日里会占用掉你很多的时间，尤其是在临近期末做大作业的时候；如果你不是学有余力的话，也请慎重考虑。但是不经历痛苦又怎能见到彩虹。我相信无论你之前的编程水平是什么样的，这门课都将是你的一次升华。选择她就是选择了挑战，过程会很痛苦，但回头看你将会感谢这一切，至少我是。

当你看到这句话的时候我已经完成了 py 大作业！ 这对话于我意义非凡 2333。从
着手选题、定题、开题，到后来的选数据、读数据、清理数据，跑模型，每一步都对
我的信心和耐心进行新一轮的考验。我从来没有独立写过一篇真正的关于实证分析的
论文，对于工作量没有太多概念。这次跟着大牛原稿的步骤一步一步做下来，才知道
这剪短几页的论文需要做多少工作。
总结一下我做这次作业遇到的困难： a.确定选题： 选题方面要么新颖要么有趣，
看似只有两个要求，但实际选择还是会有很多制约。譬如，其实我本来想分析的是美
貌与工资收入的关系，而非相貌，但难以找到可以量化的评价美貌的指标，只好转向
研究人们的外表特征这种较为易得的变量； b.读取数据：没错！读数据花了我起码
12 个小时以上的时间，因为官网上仅提供 dta 的数据，即 stata 格式， py 小白的我
尝试了多次读取都失败了，后来查文献，问大佬，最后才知道要引入与 stata 相关的
package 才能读取，好嘛终于读进来了结果运行了两三行就开始报 memory error 的错误，原来是原始数据太大内存不足，一度让我怀疑自己买了个假电脑…后来听从相
关网友的建议， 换 64bit 的 Python，然后再将原始数据拆分，这才解决了读取的问
题； c.清理数据：清理的过程较为顺畅，只是比较繁琐。因为原始数据是调查问卷＋
结果的形式，共有 1567 个 columns，所需的数据需要对照原始问卷一个一个找，因
为问卷存在很多“跳转”，找到对应的 column 较为麻烦； d.描述性统计拟合线的幂次
选择：在做男性 BMI 与工资收入的拟合线时，因为数据过于分散，一次幂发现拟合效
果较差，后来引入二次项，才得以解决； e.实证分析：回归模型的部分，由于 Python
回归统计的可视化效果不及 stata，调整了几次最后还是用 Excel 吧…d.逃不掉的计
量：原文中使用了 QR 回归，分析不同分位点上相貌对工资收入的影响，而由于我计
量水平不足再加上时间不够的原因就只能忍痛放弃…
建议及展望： 这门课修读的建议？那当然是不要选（ hhh 开个玩笑），这门课有趣
又有用怎么可以错过呢！不过，对于像我一样没有编程基础的同学还是慎选吧！或者
可以修读了 Python 程序设计后再选。当初 60 老师和我说，它可能会花费掉你其他专
业课几倍的时间，我还不信…这么倔强的结果就是， 就算每周都提前预习， 上课还是
经常一脸懵逼 o((⊙﹏⊙))o， 总的来说， 不管是期末作业还是平时小测， 这门课都
是给我最大压力的一门课(*/ω＼ *)。不过，正所谓难者不会，会者不难嘛，经过一
学期与 py 的磨合，从看不懂代码到可以写一些简单的程序、 做初步的数据清理以及
数据分析…收获还是蛮多的！ 非常感谢 60 老师每次都很耐心、温柔地解答我们的问
题，并且不断鼓励我们要相信自己的能力！没有编程基础的我也可以写上百行代码。
这门课，这次作业给我最大的感悟就是千万不要给自己设限，你想走多远就可以走多
远。最后希望自己可以成为像 60 老师一样厉害又有趣的人。
